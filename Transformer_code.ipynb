{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtq7RlAsSWMJ",
        "colab_type": "text"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwcxVfa6AKfY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8WzhW7KS25K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75BaKl3bTRDT",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esNmuEbRTFtl",
        "colab_type": "code",
        "outputId": "d97e9d53-c4f9-40a1-83d1-e17602b270d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6WE6q3WKNkT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"/content/drive/My Drive/Transformer/europarl-v7.fr-en.en\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    europarl_en = f.read()\n",
        "with open(\"/content/drive/My Drive/Transformer/europarl-v7.fr-en.fr\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    europarl_fr = f.read()\n",
        "with open(\"/content/drive/My Drive/Transformer/nonbreaking_prefix.en\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    non_breaking_prefix_en = f.read()\n",
        "with open(\"/content/drive/My Drive/Transformer/nonbreaking_prefix.fr\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    non_breaking_prefix_fr = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87lBTh6aKwkc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n",
        "non_breaking_prefix_en = [' ' + pref + '.' for pref in non_breaking_prefix_en]\n",
        "non_breaking_prefix_fr = non_breaking_prefix_fr.split(\"\\n\")\n",
        "non_breaking_prefix_fr = [' ' + pref + '.' for pref in non_breaking_prefix_fr]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-yyAnN-L5sR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_en = europarl_en\n",
        "# Add $$$ after non ending sentence points\n",
        "for prefix in non_breaking_prefix_en:\n",
        "    corpus_en = corpus_en.replace(prefix, prefix + '$$$')\n",
        "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_en)\n",
        "# Remove $$$ markers\n",
        "corpus_en = re.sub(r\".\\$\\$\\$\", '', corpus_en)\n",
        "# Clear multiple spaces\n",
        "corpus_en = re.sub(r\"  +\", \" \", corpus_en)\n",
        "corpus_en = corpus_en.split('\\n')\n",
        "\n",
        "corpus_fr = europarl_fr\n",
        "for prefix in non_breaking_prefix_fr:\n",
        "    corpus_fr = corpus_fr.replace(prefix, prefix + '$$$')\n",
        "corpus_fr = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_fr)\n",
        "corpus_fr = re.sub(r\".\\$\\$\\$\", '', corpus_fr)\n",
        "corpus_fr = re.sub(r\"  +\", \" \", corpus_fr)\n",
        "corpus_fr = corpus_fr.split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys2kJXbgO9Yt",
        "colab_type": "text"
      },
      "source": [
        "# Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvwXyxDGN_VM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_en, target_vocab_size=2**13)\n",
        "tokenizer_fr = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_fr, target_vocab_size=2**13)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKgukdS_Pc7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "pickle.dump(tokenizer_en,open('/content/drive/My Drive/Transformer/tokenizer_eng','wb'))\n",
        "pickle.dump(tokenizer_fr,open('/content/drive/My Drive/Transformer/tokenizer_fr','wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IZ8UjIGOG2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "tokenizer_en = pickle.load(open('/content/drive/My Drive/Transformer/tokenizer_eng','rb'))\n",
        "tokenizer_fr = pickle.load(open('/content/drive/My Drive/Transformer/tokenizer_fr','rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7NGRYpKTdIt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2\n",
        "VOCAB_SIZE_FR = tokenizer_fr.vocab_size + 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRsGTUwoQWjE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_en = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence)  + [VOCAB_SIZE_EN-1] for sentence in corpus_en]\n",
        "sentence_fr = [[VOCAB_SIZE_FR-2] + tokenizer_fr.encode(sentence)  + [VOCAB_SIZE_FR-1] for sentence in corpus_fr]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBiiKe0qTVfS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN =20\n",
        "idx_to_remove = [count for count,sentence in enumerate(sentence_en) if len(sentence)> MAX_LEN ]\n",
        "for idx in reversed(idx_to_remove):\n",
        "  del sentence_en[idx]\n",
        "  del sentence_fr[idx]\n",
        "idx_to_remove = [count for count,sentence in enumerate(sentence_fr) if len(sentence)> MAX_LEN ]\n",
        "for idx in reversed(idx_to_remove):\n",
        "  del sentence_en[idx]\n",
        "  del sentence_fr[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DzB3Wx1VYZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs =tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    sentence_en, maxlen=MAX_LEN, padding='post',\n",
        "    value=0\n",
        ")\n",
        "outputs =tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    sentence_fr, maxlen=MAX_LEN, padding='post',\n",
        "    value=0\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EytnfK6PZy16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FusWpzHHWBgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "pickle.dump(inputs,open('/content/drive/My Drive/Transformer/processed_inputs','wb'))\n",
        "pickle.dump(outputs,open('/content/drive/My Drive/Transformer/processed_outputs','wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAv6SHbEOS3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "inputs = pickle.load(open('/content/drive/My Drive/Transformer/processed_inputs','rb'))\n",
        "outputs = pickle.load(open('/content/drive/My Drive/Transformer/processed_outputs','rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dEsMAf1XJU5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs,outputs))\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKGZGK_8dAZ6",
        "colab_type": "text"
      },
      "source": [
        "# CREATE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYIQCeLObrYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "    \n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n",
        "        return pos * angles\n",
        "\n",
        "    def call(self, inputs):\n",
        "        seq_length = inputs.shape.as_list()[-2]\n",
        "        d_model = inputs.shape.as_list()[-1]\n",
        "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
        "                                 np.arange(d_model)[np.newaxis, :],\n",
        "                                 d_model)\n",
        "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "        pos_encoding = angles[np.newaxis, ...]\n",
        "        return inputs + tf.cast(pos_encoding, tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03NMIsc0tYDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "    product = tf.matmul(queries, keys, transpose_b=True)\n",
        "    \n",
        "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
        "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
        "    \n",
        "    if mask is not None:\n",
        "        scaled_product += (mask * -1e9)\n",
        "    \n",
        "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
        "    \n",
        "    return attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7qnuCkO9HIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "    \n",
        "    def __init__(self, nb_proj):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.nb_proj = nb_proj\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        assert self.d_model % self.nb_proj == 0\n",
        "        \n",
        "        self.d_proj = self.d_model // self.nb_proj\n",
        "        \n",
        "        self.query_lin = layers.Dense(units=self.d_model)\n",
        "        self.key_lin = layers.Dense(units=self.d_model)\n",
        "        self.value_lin = layers.Dense(units=self.d_model)\n",
        "        \n",
        "        self.final_lin = layers.Dense(units=self.d_model)\n",
        "        \n",
        "    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\n",
        "        shape = (batch_size,\n",
        "                 -1,\n",
        "                 self.nb_proj,\n",
        "                 self.d_proj)\n",
        "        splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)\n",
        "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\n",
        "    \n",
        "    def call(self, queries, keys, values, mask):\n",
        "        batch_size = tf.shape(queries)[0]\n",
        "        \n",
        "        queries = self.query_lin(queries)\n",
        "        keys = self.key_lin(keys)\n",
        "        values = self.value_lin(values)\n",
        "        \n",
        "        queries = self.split_proj(queries, batch_size)\n",
        "        keys = self.split_proj(keys, batch_size)\n",
        "        values = self.split_proj(values, batch_size)\n",
        "        \n",
        "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
        "        \n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        \n",
        "        concat_attention = tf.reshape(attention,\n",
        "                                      shape=(batch_size, -1, self.d_model))\n",
        "        \n",
        "        outputs = self.final_lin(concat_attention)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqOR8PfHNysY",
        "colab_type": "text"
      },
      "source": [
        "## ENCODER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drTHjsuuJO5D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        \n",
        "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def call(self, inputs, mask, training):\n",
        "        attention = self.multi_head_attention(inputs,\n",
        "                                              inputs,\n",
        "                                              inputs,\n",
        "                                              mask)\n",
        "        attention = self.dropout_1(attention, training=training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        \n",
        "        outputs = self.dense_1(attention)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_2(outputs, training=training)\n",
        "        outputs = self.norm_2(outputs + attention)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwNdUPHSQjpb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"encoder\"):\n",
        "        super(Encoder, self).__init__(name=name)\n",
        "        self.nb_layers = nb_layers\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        self.enc_layers = [EncoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout_rate) \n",
        "                           for _ in range(nb_layers)]\n",
        "    \n",
        "    def call(self, inputs, mask, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        \n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.enc_layers[i](outputs, mask, training)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mYfnCtHQ-Ro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        \n",
        "        # Self multi head attention\n",
        "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Multi head attention combined with encoder output\n",
        "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Feed foward\n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units,\n",
        "                                    activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        attention = self.multi_head_attention_1(inputs,\n",
        "                                                inputs,\n",
        "                                                inputs,\n",
        "                                                mask_1)\n",
        "        attention = self.dropout_1(attention, training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        \n",
        "        attention_2 = self.multi_head_attention_2(attention,\n",
        "                                                  enc_outputs,\n",
        "                                                  enc_outputs,\n",
        "                                                  mask_2)\n",
        "        attention_2 = self.dropout_2(attention_2, training)\n",
        "        attention_2 = self.norm_2(attention_2 + attention)\n",
        "        \n",
        "        outputs = self.dense_1(attention_2)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_3(outputs, training)\n",
        "        outputs = self.norm_3(outputs + attention_2)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gYPiFjrWZUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"decoder\"):\n",
        "        super(Decoder, self).__init__(name=name)\n",
        "        self.d_model = d_model\n",
        "        self.nb_layers = nb_layers\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        \n",
        "        self.dec_layers = [DecoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout_rate) \n",
        "                           for i in range(nb_layers)]\n",
        "    \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        \n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.dec_layers[i](outputs,\n",
        "                                         enc_outputs,\n",
        "                                         mask_1,\n",
        "                                         mask_2,\n",
        "                                         training)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THvb0WfVZM78",
        "colab_type": "text"
      },
      "source": [
        "# TRANSFORMER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YQJ22qVmA3x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8e4d7ca0-9575-40a7-c881-d4b17ef6eedb"
      },
      "source": [
        "'''a = [[1,2.0,3,0,0],[4,1,3,0,0],[1,3,3,2,0],[1,3,3,2,0]]\n",
        "m =tf.cast(tf.math.equal(a,0),dtype=tf.float32)\n",
        "m[:,tf.newaxis,tf.newaxis,:]'''"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a = [[1,2.0,3,0,0],[4,1,3,0,0],[1,3,3,2,0],[1,3,3,2,0]]\\nm =tf.cast(tf.math.equal(a,0),dtype=tf.float32)\\nm[:,tf.newaxis,tf.newaxis,:]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBsx17jQx5Or",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1 - tf.linalg.band_part(a, -1, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mp9N01ixiSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tf.maximum(\n",
        "#             m[:,tf.newaxis,tf.newaxis,:],\n",
        "#             1-tf.linalg.band_part(a, -1, 0)\n",
        "#         )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJfcz_InZJsx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocab_size_enc,\n",
        "                 vocab_size_dec,\n",
        "                 d_model,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 name=\"transformer\"):\n",
        "        super(Transformer, self).__init__(name=name)\n",
        "        \n",
        "        self.encoder = Encoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_enc,\n",
        "                               d_model)\n",
        "        self.decoder = Decoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_dec,\n",
        "                               d_model)\n",
        "        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"lin_ouput\")\n",
        "    \n",
        "    def create_padding_mask(self, seq):\n",
        "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    def create_look_ahead_mask(self, seq):\n",
        "        seq_len = tf.shape(seq)[1]\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "        return look_ahead_mask\n",
        "    \n",
        "    def call(self, enc_inputs, dec_inputs, training):\n",
        "        enc_mask = self.create_padding_mask(enc_inputs)\n",
        "        dec_mask_1 = tf.maximum(\n",
        "            self.create_padding_mask(dec_inputs),\n",
        "            self.create_look_ahead_mask(dec_inputs)\n",
        "        )\n",
        "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "        \n",
        "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "        dec_outputs = self.decoder(dec_inputs,\n",
        "                                   enc_outputs,\n",
        "                                   dec_mask_1,\n",
        "                                   dec_mask_2,\n",
        "                                   training)\n",
        "        \n",
        "        outputs = self.last_linear(dec_outputs)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMPFantSMapd",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlOqho0tZCf_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Hyper-parameters\n",
        "D_MODEL = 128 # 512\n",
        "NB_LAYERS = 4 # 6\n",
        "FFN_UNITS = 512 # 2048\n",
        "NB_PROJ = 8 # 8\n",
        "DROPOUT_RATE = 0.1 # 0.1\n",
        "\n",
        "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
        "                          vocab_size_dec=VOCAB_SIZE_FR,\n",
        "                          d_model=D_MODEL,\n",
        "                          nb_layers=NB_LAYERS,\n",
        "                          FFN_units=FFN_UNITS,\n",
        "                          nb_proj=NB_PROJ,\n",
        "                          dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGxMcUU5UBV_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a3a451a-83d8-4ba7-bf42-800904629e39"
      },
      "source": [
        "tf.logical_not(tf.math.equal([1,23,4,5,0,0,0],0))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(7,), dtype=bool, numpy=array([ True,  True,  True,  True, False, False, False])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gN6efIrN9EY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction=\"none\")\n",
        "\n",
        "def loss_function(target, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "    loss_ = loss_object(target, pred)\n",
        "    \n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    \n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYZqWaW0hBk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        \n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "        \n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "leaning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(leaning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m4PEDYT-h4X",
        "colab_type": "text"
      },
      "source": [
        "# Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcT0mX9Gle_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"/content/drive/My Drive/Transformer/cktps/\"\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=transformer)\n",
        "manager = tf.train.CheckpointManager(checkpoint, directory=checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if manager.latest_checkpoint:\n",
        "  status= checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))\n",
        "  print(\" LATEST CHECKPOINT RESTORED !!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rm9WkOUa9cw6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# checkpoint_path_wts = \"/content/drive/My Drive/Transformer/ckpt_weights/cp-{epoch:04d}.ckpt\"\n",
        "# checkpoint_dir = os.path.dirname(checkpoint_path_wts)\n",
        "\n",
        "# # Create a callback that saves the model's weights\n",
        "# cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path_wts,\n",
        "#                                                  save_weights_only=True,\n",
        "#                                                  verbose=1)\n",
        "\n",
        "# transformer.save_weights(checkpoint_path.format(epoch=0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWfN46Rn-k9e",
        "colab_type": "text"
      },
      "source": [
        "# Run Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyCHYqvIm7wa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "db53d6e0-9895-4882-8aad-bece169d851b"
      },
      "source": [
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"Start of epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    \n",
        "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
        "        dec_inputs = targets[:, :-1]\n",
        "        dec_outputs_real = targets[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "            loss = loss_function(dec_outputs_real, predictions)\n",
        "        \n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "        \n",
        "        train_loss(loss)\n",
        "        train_accuracy(dec_outputs_real, predictions)\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
        "                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
        "            \n",
        "    ckpt_save_path = manager.save()\n",
        "    print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1,\n",
        "                                                        ckpt_save_path))\n",
        "    print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of epoch 1\n",
            "Epoch 1 Batch 0 Loss 6.1219 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 6.2428 Accuracy 0.0065\n",
            "Epoch 1 Batch 100 Loss 6.1683 Accuracy 0.0284\n",
            "Epoch 1 Batch 150 Loss 6.1094 Accuracy 0.0366\n",
            "Epoch 1 Batch 200 Loss 6.0272 Accuracy 0.0406\n",
            "Epoch 1 Batch 250 Loss 5.9204 Accuracy 0.0443\n",
            "Epoch 1 Batch 300 Loss 5.7951 Accuracy 0.0516\n",
            "Epoch 1 Batch 350 Loss 5.6695 Accuracy 0.0574\n",
            "Epoch 1 Batch 400 Loss 5.5431 Accuracy 0.0620\n",
            "Epoch 1 Batch 450 Loss 5.4254 Accuracy 0.0656\n",
            "Epoch 1 Batch 500 Loss 5.3194 Accuracy 0.0701\n",
            "Epoch 1 Batch 550 Loss 5.2214 Accuracy 0.0751\n",
            "Epoch 1 Batch 600 Loss 5.1269 Accuracy 0.0804\n",
            "Epoch 1 Batch 650 Loss 5.0327 Accuracy 0.0856\n",
            "Epoch 1 Batch 700 Loss 4.9432 Accuracy 0.0907\n",
            "Epoch 1 Batch 750 Loss 4.8578 Accuracy 0.0958\n",
            "Epoch 1 Batch 800 Loss 4.7799 Accuracy 0.1009\n",
            "Epoch 1 Batch 850 Loss 4.7045 Accuracy 0.1059\n",
            "Epoch 1 Batch 900 Loss 4.6319 Accuracy 0.1106\n",
            "Epoch 1 Batch 950 Loss 4.5603 Accuracy 0.1152\n",
            "Epoch 1 Batch 1000 Loss 4.4961 Accuracy 0.1195\n",
            "Epoch 1 Batch 1050 Loss 4.4364 Accuracy 0.1236\n",
            "Epoch 1 Batch 1100 Loss 4.3786 Accuracy 0.1273\n",
            "Epoch 1 Batch 1150 Loss 4.3228 Accuracy 0.1309\n",
            "Epoch 1 Batch 1200 Loss 4.2711 Accuracy 0.1342\n",
            "Epoch 1 Batch 1250 Loss 4.2214 Accuracy 0.1375\n",
            "Epoch 1 Batch 1300 Loss 4.1736 Accuracy 0.1405\n",
            "Epoch 1 Batch 1350 Loss 4.1301 Accuracy 0.1436\n",
            "Epoch 1 Batch 1400 Loss 4.0868 Accuracy 0.1468\n",
            "Epoch 1 Batch 1450 Loss 4.0451 Accuracy 0.1497\n",
            "Epoch 1 Batch 1500 Loss 4.0078 Accuracy 0.1528\n",
            "Epoch 1 Batch 1550 Loss 3.9709 Accuracy 0.1558\n",
            "Epoch 1 Batch 1600 Loss 3.9347 Accuracy 0.1586\n",
            "Epoch 1 Batch 1650 Loss 3.8999 Accuracy 0.1615\n",
            "Epoch 1 Batch 1700 Loss 3.8664 Accuracy 0.1641\n",
            "Epoch 1 Batch 1750 Loss 3.8352 Accuracy 0.1668\n",
            "Epoch 1 Batch 1800 Loss 3.8040 Accuracy 0.1695\n",
            "Epoch 1 Batch 1850 Loss 3.7746 Accuracy 0.1720\n",
            "Epoch 1 Batch 1900 Loss 3.7460 Accuracy 0.1745\n",
            "Epoch 1 Batch 1950 Loss 3.7184 Accuracy 0.1770\n",
            "Epoch 1 Batch 2000 Loss 3.6910 Accuracy 0.1794\n",
            "Epoch 1 Batch 2050 Loss 3.6633 Accuracy 0.1814\n",
            "Epoch 1 Batch 2100 Loss 3.6370 Accuracy 0.1834\n",
            "Epoch 1 Batch 2150 Loss 3.6106 Accuracy 0.1854\n",
            "Epoch 1 Batch 2200 Loss 3.5841 Accuracy 0.1874\n",
            "Epoch 1 Batch 2250 Loss 3.5583 Accuracy 0.1894\n",
            "Epoch 1 Batch 2300 Loss 3.5332 Accuracy 0.1912\n",
            "Epoch 1 Batch 2350 Loss 3.5081 Accuracy 0.1932\n",
            "Epoch 1 Batch 2400 Loss 3.4844 Accuracy 0.1951\n",
            "Epoch 1 Batch 2450 Loss 3.4599 Accuracy 0.1970\n",
            "Epoch 1 Batch 2500 Loss 3.4369 Accuracy 0.1989\n",
            "Epoch 1 Batch 2550 Loss 3.4142 Accuracy 0.2009\n",
            "Epoch 1 Batch 2600 Loss 3.3912 Accuracy 0.2028\n",
            "Epoch 1 Batch 2650 Loss 3.3692 Accuracy 0.2048\n",
            "Epoch 1 Batch 2700 Loss 3.3468 Accuracy 0.2067\n",
            "Epoch 1 Batch 2750 Loss 3.3255 Accuracy 0.2087\n",
            "Epoch 1 Batch 2800 Loss 3.3048 Accuracy 0.2108\n",
            "Epoch 1 Batch 2850 Loss 3.2843 Accuracy 0.2127\n",
            "Epoch 1 Batch 2900 Loss 3.2643 Accuracy 0.2146\n",
            "Epoch 1 Batch 2950 Loss 3.2444 Accuracy 0.2165\n",
            "Epoch 1 Batch 3000 Loss 3.2251 Accuracy 0.2183\n",
            "Epoch 1 Batch 3050 Loss 3.2059 Accuracy 0.2202\n",
            "Epoch 1 Batch 3100 Loss 3.1870 Accuracy 0.2220\n",
            "Epoch 1 Batch 3150 Loss 3.1676 Accuracy 0.2239\n",
            "Epoch 1 Batch 3200 Loss 3.1488 Accuracy 0.2257\n",
            "Epoch 1 Batch 3250 Loss 3.1305 Accuracy 0.2275\n",
            "Epoch 1 Batch 3300 Loss 3.1122 Accuracy 0.2294\n",
            "Epoch 1 Batch 3350 Loss 3.0944 Accuracy 0.2312\n",
            "Epoch 1 Batch 3400 Loss 3.0770 Accuracy 0.2330\n",
            "Epoch 1 Batch 3450 Loss 3.0596 Accuracy 0.2349\n",
            "Epoch 1 Batch 3500 Loss 3.0431 Accuracy 0.2366\n",
            "Epoch 1 Batch 3550 Loss 3.0263 Accuracy 0.2384\n",
            "Epoch 1 Batch 3600 Loss 3.0104 Accuracy 0.2402\n",
            "Epoch 1 Batch 3650 Loss 2.9943 Accuracy 0.2420\n",
            "Epoch 1 Batch 3700 Loss 2.9789 Accuracy 0.2437\n",
            "Epoch 1 Batch 3750 Loss 2.9633 Accuracy 0.2454\n",
            "Epoch 1 Batch 3800 Loss 2.9482 Accuracy 0.2471\n",
            "Epoch 1 Batch 3850 Loss 2.9339 Accuracy 0.2488\n",
            "Epoch 1 Batch 3900 Loss 2.9194 Accuracy 0.2505\n",
            "Epoch 1 Batch 3950 Loss 2.9049 Accuracy 0.2522\n",
            "Epoch 1 Batch 4000 Loss 2.8908 Accuracy 0.2538\n",
            "Epoch 1 Batch 4050 Loss 2.8772 Accuracy 0.2555\n",
            "Epoch 1 Batch 4100 Loss 2.8635 Accuracy 0.2570\n",
            "Epoch 1 Batch 4150 Loss 2.8505 Accuracy 0.2585\n",
            "Epoch 1 Batch 4200 Loss 2.8383 Accuracy 0.2598\n",
            "Epoch 1 Batch 4250 Loss 2.8262 Accuracy 0.2612\n",
            "Epoch 1 Batch 4300 Loss 2.8148 Accuracy 0.2624\n",
            "Epoch 1 Batch 4350 Loss 2.8037 Accuracy 0.2636\n",
            "Epoch 1 Batch 4400 Loss 2.7930 Accuracy 0.2647\n",
            "Epoch 1 Batch 4450 Loss 2.7823 Accuracy 0.2659\n",
            "Epoch 1 Batch 4500 Loss 2.7718 Accuracy 0.2670\n",
            "Epoch 1 Batch 4550 Loss 2.7618 Accuracy 0.2681\n",
            "Epoch 1 Batch 4600 Loss 2.7520 Accuracy 0.2692\n",
            "Epoch 1 Batch 4650 Loss 2.7425 Accuracy 0.2703\n",
            "Epoch 1 Batch 4700 Loss 2.7327 Accuracy 0.2713\n",
            "Epoch 1 Batch 4750 Loss 2.7230 Accuracy 0.2724\n",
            "Epoch 1 Batch 4800 Loss 2.7134 Accuracy 0.2734\n",
            "Epoch 1 Batch 4850 Loss 2.7037 Accuracy 0.2745\n",
            "Epoch 1 Batch 4900 Loss 2.6945 Accuracy 0.2755\n",
            "Epoch 1 Batch 4950 Loss 2.6852 Accuracy 0.2765\n",
            "Epoch 1 Batch 5000 Loss 2.6764 Accuracy 0.2775\n",
            "Epoch 1 Batch 5050 Loss 2.6674 Accuracy 0.2785\n",
            "Epoch 1 Batch 5100 Loss 2.6586 Accuracy 0.2794\n",
            "Epoch 1 Batch 5150 Loss 2.6499 Accuracy 0.2804\n",
            "Epoch 1 Batch 5200 Loss 2.6412 Accuracy 0.2812\n",
            "Epoch 1 Batch 5250 Loss 2.6325 Accuracy 0.2821\n",
            "Epoch 1 Batch 5300 Loss 2.6239 Accuracy 0.2829\n",
            "Epoch 1 Batch 5350 Loss 2.6156 Accuracy 0.2838\n",
            "Epoch 1 Batch 5400 Loss 2.6071 Accuracy 0.2846\n",
            "Epoch 1 Batch 5450 Loss 2.5986 Accuracy 0.2855\n",
            "Epoch 1 Batch 5500 Loss 2.5901 Accuracy 0.2863\n",
            "Epoch 1 Batch 5550 Loss 2.5819 Accuracy 0.2871\n",
            "Epoch 1 Batch 5600 Loss 2.5738 Accuracy 0.2879\n",
            "Epoch 1 Batch 5650 Loss 2.5658 Accuracy 0.2888\n",
            "Epoch 1 Batch 5700 Loss 2.5578 Accuracy 0.2896\n",
            "Saving checkpoint for epoch 1 at /content/drive/My Drive/Transformer/cktps/ckpt-1\n",
            "Time taken for 1 epoch: 1438.9913516044617 secs\n",
            "\n",
            "Start of epoch 2\n",
            "Epoch 2 Batch 0 Loss 1.7636 Accuracy 0.3618\n",
            "Epoch 2 Batch 50 Loss 1.7051 Accuracy 0.3837\n",
            "Epoch 2 Batch 100 Loss 1.6889 Accuracy 0.3840\n",
            "Epoch 2 Batch 150 Loss 1.6837 Accuracy 0.3856\n",
            "Epoch 2 Batch 200 Loss 1.6741 Accuracy 0.3880\n",
            "Epoch 2 Batch 250 Loss 1.6668 Accuracy 0.3899\n",
            "Epoch 2 Batch 300 Loss 1.6631 Accuracy 0.3904\n",
            "Epoch 2 Batch 350 Loss 1.6549 Accuracy 0.3908\n",
            "Epoch 2 Batch 400 Loss 1.6511 Accuracy 0.3910\n",
            "Epoch 2 Batch 450 Loss 1.6438 Accuracy 0.3915\n",
            "Epoch 2 Batch 500 Loss 1.6365 Accuracy 0.3918\n",
            "Epoch 2 Batch 550 Loss 1.6329 Accuracy 0.3921\n",
            "Epoch 2 Batch 600 Loss 1.6295 Accuracy 0.3926\n",
            "Epoch 2 Batch 650 Loss 1.6284 Accuracy 0.3927\n",
            "Epoch 2 Batch 700 Loss 1.6264 Accuracy 0.3933\n",
            "Epoch 2 Batch 750 Loss 1.6220 Accuracy 0.3940\n",
            "Epoch 2 Batch 800 Loss 1.6174 Accuracy 0.3946\n",
            "Epoch 2 Batch 850 Loss 1.6135 Accuracy 0.3952\n",
            "Epoch 2 Batch 900 Loss 1.6103 Accuracy 0.3957\n",
            "Epoch 2 Batch 950 Loss 1.6057 Accuracy 0.3963\n",
            "Epoch 2 Batch 1000 Loss 1.6017 Accuracy 0.3964\n",
            "Epoch 2 Batch 1050 Loss 1.5981 Accuracy 0.3967\n",
            "Epoch 2 Batch 1100 Loss 1.5950 Accuracy 0.3969\n",
            "Epoch 2 Batch 1150 Loss 1.5924 Accuracy 0.3971\n",
            "Epoch 2 Batch 1200 Loss 1.5898 Accuracy 0.3974\n",
            "Epoch 2 Batch 1250 Loss 1.5865 Accuracy 0.3978\n",
            "Epoch 2 Batch 1300 Loss 1.5827 Accuracy 0.3985\n",
            "Epoch 2 Batch 1350 Loss 1.5805 Accuracy 0.3993\n",
            "Epoch 2 Batch 1400 Loss 1.5768 Accuracy 0.4002\n",
            "Epoch 2 Batch 1450 Loss 1.5716 Accuracy 0.4011\n",
            "Epoch 2 Batch 1500 Loss 1.5672 Accuracy 0.4022\n",
            "Epoch 2 Batch 1550 Loss 1.5621 Accuracy 0.4033\n",
            "Epoch 2 Batch 1600 Loss 1.5585 Accuracy 0.4042\n",
            "Epoch 2 Batch 1650 Loss 1.5552 Accuracy 0.4053\n",
            "Epoch 2 Batch 1700 Loss 1.5515 Accuracy 0.4062\n",
            "Epoch 2 Batch 1750 Loss 1.5477 Accuracy 0.4072\n",
            "Epoch 2 Batch 1800 Loss 1.5439 Accuracy 0.4082\n",
            "Epoch 2 Batch 1850 Loss 1.5399 Accuracy 0.4092\n",
            "Epoch 2 Batch 1900 Loss 1.5361 Accuracy 0.4102\n",
            "Epoch 2 Batch 1950 Loss 1.5319 Accuracy 0.4112\n",
            "Epoch 2 Batch 2000 Loss 1.5279 Accuracy 0.4119\n",
            "Epoch 2 Batch 2050 Loss 1.5240 Accuracy 0.4126\n",
            "Epoch 2 Batch 2100 Loss 1.5202 Accuracy 0.4132\n",
            "Epoch 2 Batch 2150 Loss 1.5156 Accuracy 0.4137\n",
            "Epoch 2 Batch 2200 Loss 1.5109 Accuracy 0.4142\n",
            "Epoch 2 Batch 2250 Loss 1.5065 Accuracy 0.4147\n",
            "Epoch 2 Batch 2300 Loss 1.5017 Accuracy 0.4152\n",
            "Epoch 2 Batch 2350 Loss 1.4965 Accuracy 0.4156\n",
            "Epoch 2 Batch 2400 Loss 1.4916 Accuracy 0.4161\n",
            "Epoch 2 Batch 2450 Loss 1.4868 Accuracy 0.4166\n",
            "Epoch 2 Batch 2500 Loss 1.4831 Accuracy 0.4172\n",
            "Epoch 2 Batch 2550 Loss 1.4783 Accuracy 0.4178\n",
            "Epoch 2 Batch 2600 Loss 1.4740 Accuracy 0.4183\n",
            "Epoch 2 Batch 2650 Loss 1.4695 Accuracy 0.4188\n",
            "Epoch 2 Batch 2700 Loss 1.4650 Accuracy 0.4194\n",
            "Epoch 2 Batch 2750 Loss 1.4605 Accuracy 0.4201\n",
            "Epoch 2 Batch 2800 Loss 1.4554 Accuracy 0.4207\n",
            "Epoch 2 Batch 2850 Loss 1.4518 Accuracy 0.4212\n",
            "Epoch 2 Batch 2900 Loss 1.4477 Accuracy 0.4216\n",
            "Epoch 2 Batch 2950 Loss 1.4436 Accuracy 0.4222\n",
            "Epoch 2 Batch 3000 Loss 1.4401 Accuracy 0.4227\n",
            "Epoch 2 Batch 3050 Loss 1.4363 Accuracy 0.4232\n",
            "Epoch 2 Batch 3100 Loss 1.4326 Accuracy 0.4238\n",
            "Epoch 2 Batch 3150 Loss 1.4293 Accuracy 0.4242\n",
            "Epoch 2 Batch 3200 Loss 1.4255 Accuracy 0.4247\n",
            "Epoch 2 Batch 3250 Loss 1.4219 Accuracy 0.4253\n",
            "Epoch 2 Batch 3300 Loss 1.4184 Accuracy 0.4258\n",
            "Epoch 2 Batch 3350 Loss 1.4150 Accuracy 0.4263\n",
            "Epoch 2 Batch 3400 Loss 1.4113 Accuracy 0.4269\n",
            "Epoch 2 Batch 3450 Loss 1.4074 Accuracy 0.4275\n",
            "Epoch 2 Batch 3500 Loss 1.4038 Accuracy 0.4281\n",
            "Epoch 2 Batch 3550 Loss 1.4008 Accuracy 0.4286\n",
            "Epoch 2 Batch 3600 Loss 1.3972 Accuracy 0.4291\n",
            "Epoch 2 Batch 3650 Loss 1.3941 Accuracy 0.4297\n",
            "Epoch 2 Batch 3700 Loss 1.3908 Accuracy 0.4303\n",
            "Epoch 2 Batch 3750 Loss 1.3876 Accuracy 0.4308\n",
            "Epoch 2 Batch 3800 Loss 1.3849 Accuracy 0.4314\n",
            "Epoch 2 Batch 3850 Loss 1.3818 Accuracy 0.4319\n",
            "Epoch 2 Batch 3900 Loss 1.3788 Accuracy 0.4325\n",
            "Epoch 2 Batch 3950 Loss 1.3759 Accuracy 0.4331\n",
            "Epoch 2 Batch 4000 Loss 1.3733 Accuracy 0.4336\n",
            "Epoch 2 Batch 4050 Loss 1.3702 Accuracy 0.4341\n",
            "Epoch 2 Batch 4100 Loss 1.3676 Accuracy 0.4345\n",
            "Epoch 2 Batch 4150 Loss 1.3658 Accuracy 0.4348\n",
            "Epoch 2 Batch 4200 Loss 1.3650 Accuracy 0.4350\n",
            "Epoch 2 Batch 4250 Loss 1.3637 Accuracy 0.4352\n",
            "Epoch 2 Batch 4300 Loss 1.3631 Accuracy 0.4353\n",
            "Epoch 2 Batch 4350 Loss 1.3628 Accuracy 0.4355\n",
            "Epoch 2 Batch 4400 Loss 1.3625 Accuracy 0.4355\n",
            "Epoch 2 Batch 4450 Loss 1.3621 Accuracy 0.4356\n",
            "Epoch 2 Batch 4500 Loss 1.3621 Accuracy 0.4356\n",
            "Epoch 2 Batch 4550 Loss 1.3622 Accuracy 0.4357\n",
            "Epoch 2 Batch 4600 Loss 1.3622 Accuracy 0.4356\n",
            "Epoch 2 Batch 4650 Loss 1.3624 Accuracy 0.4357\n",
            "Epoch 2 Batch 4700 Loss 1.3621 Accuracy 0.4357\n",
            "Epoch 2 Batch 4750 Loss 1.3625 Accuracy 0.4356\n",
            "Epoch 2 Batch 4800 Loss 1.3624 Accuracy 0.4356\n",
            "Epoch 2 Batch 4850 Loss 1.3624 Accuracy 0.4356\n",
            "Epoch 2 Batch 4900 Loss 1.3621 Accuracy 0.4357\n",
            "Epoch 2 Batch 4950 Loss 1.3622 Accuracy 0.4357\n",
            "Epoch 2 Batch 5000 Loss 1.3621 Accuracy 0.4357\n",
            "Epoch 2 Batch 5050 Loss 1.3621 Accuracy 0.4357\n",
            "Epoch 2 Batch 5100 Loss 1.3624 Accuracy 0.4356\n",
            "Epoch 2 Batch 5150 Loss 1.3624 Accuracy 0.4356\n",
            "Epoch 2 Batch 5200 Loss 1.3626 Accuracy 0.4355\n",
            "Epoch 2 Batch 5250 Loss 1.3627 Accuracy 0.4355\n",
            "Epoch 2 Batch 5300 Loss 1.3625 Accuracy 0.4354\n",
            "Epoch 2 Batch 5350 Loss 1.3623 Accuracy 0.4353\n",
            "Epoch 2 Batch 5400 Loss 1.3621 Accuracy 0.4352\n",
            "Epoch 2 Batch 5450 Loss 1.3623 Accuracy 0.4351\n",
            "Epoch 2 Batch 5500 Loss 1.3620 Accuracy 0.4350\n",
            "Epoch 2 Batch 5550 Loss 1.3617 Accuracy 0.4350\n",
            "Epoch 2 Batch 5600 Loss 1.3615 Accuracy 0.4350\n",
            "Epoch 2 Batch 5650 Loss 1.3610 Accuracy 0.4349\n",
            "Epoch 2 Batch 5700 Loss 1.3609 Accuracy 0.4349\n",
            "Saving checkpoint for epoch 2 at /content/drive/My Drive/Transformer/cktps/ckpt-2\n",
            "Time taken for 1 epoch: 1436.1296446323395 secs\n",
            "\n",
            "Start of epoch 3\n",
            "Epoch 3 Batch 0 Loss 1.4792 Accuracy 0.4293\n",
            "Epoch 3 Batch 50 Loss 1.3450 Accuracy 0.4338\n",
            "Epoch 3 Batch 100 Loss 1.3244 Accuracy 0.4375\n",
            "Epoch 3 Batch 150 Loss 1.3338 Accuracy 0.4381\n",
            "Epoch 3 Batch 200 Loss 1.3309 Accuracy 0.4393\n",
            "Epoch 3 Batch 250 Loss 1.3228 Accuracy 0.4392\n",
            "Epoch 3 Batch 300 Loss 1.3219 Accuracy 0.4387\n",
            "Epoch 3 Batch 350 Loss 1.3208 Accuracy 0.4394\n",
            "Epoch 3 Batch 400 Loss 1.3210 Accuracy 0.4394\n",
            "Epoch 3 Batch 450 Loss 1.3177 Accuracy 0.4391\n",
            "Epoch 3 Batch 500 Loss 1.3130 Accuracy 0.4393\n",
            "Epoch 3 Batch 550 Loss 1.3092 Accuracy 0.4393\n",
            "Epoch 3 Batch 600 Loss 1.3095 Accuracy 0.4395\n",
            "Epoch 3 Batch 650 Loss 1.3095 Accuracy 0.4400\n",
            "Epoch 3 Batch 700 Loss 1.3093 Accuracy 0.4402\n",
            "Epoch 3 Batch 750 Loss 1.3073 Accuracy 0.4408\n",
            "Epoch 3 Batch 800 Loss 1.3064 Accuracy 0.4411\n",
            "Epoch 3 Batch 850 Loss 1.3040 Accuracy 0.4413\n",
            "Epoch 3 Batch 900 Loss 1.3027 Accuracy 0.4416\n",
            "Epoch 3 Batch 950 Loss 1.3000 Accuracy 0.4418\n",
            "Epoch 3 Batch 1000 Loss 1.2975 Accuracy 0.4420\n",
            "Epoch 3 Batch 1050 Loss 1.2956 Accuracy 0.4422\n",
            "Epoch 3 Batch 1100 Loss 1.2940 Accuracy 0.4422\n",
            "Epoch 3 Batch 1150 Loss 1.2925 Accuracy 0.4423\n",
            "Epoch 3 Batch 1200 Loss 1.2900 Accuracy 0.4428\n",
            "Epoch 3 Batch 1250 Loss 1.2887 Accuracy 0.4431\n",
            "Epoch 3 Batch 1300 Loss 1.2862 Accuracy 0.4435\n",
            "Epoch 3 Batch 1350 Loss 1.2835 Accuracy 0.4438\n",
            "Epoch 3 Batch 1400 Loss 1.2806 Accuracy 0.4444\n",
            "Epoch 3 Batch 1450 Loss 1.2786 Accuracy 0.4453\n",
            "Epoch 3 Batch 1500 Loss 1.2754 Accuracy 0.4460\n",
            "Epoch 3 Batch 1550 Loss 1.2718 Accuracy 0.4471\n",
            "Epoch 3 Batch 1600 Loss 1.2688 Accuracy 0.4479\n",
            "Epoch 3 Batch 1650 Loss 1.2662 Accuracy 0.4488\n",
            "Epoch 3 Batch 1700 Loss 1.2636 Accuracy 0.4497\n",
            "Epoch 3 Batch 1750 Loss 1.2611 Accuracy 0.4504\n",
            "Epoch 3 Batch 1800 Loss 1.2583 Accuracy 0.4512\n",
            "Epoch 3 Batch 1850 Loss 1.2555 Accuracy 0.4521\n",
            "Epoch 3 Batch 1900 Loss 1.2527 Accuracy 0.4529\n",
            "Epoch 3 Batch 1950 Loss 1.2512 Accuracy 0.4537\n",
            "Epoch 3 Batch 2000 Loss 1.2488 Accuracy 0.4544\n",
            "Epoch 3 Batch 2050 Loss 1.2467 Accuracy 0.4549\n",
            "Epoch 3 Batch 2100 Loss 1.2441 Accuracy 0.4552\n",
            "Epoch 3 Batch 2150 Loss 1.2404 Accuracy 0.4556\n",
            "Epoch 3 Batch 2200 Loss 1.2368 Accuracy 0.4559\n",
            "Epoch 3 Batch 2250 Loss 1.2331 Accuracy 0.4561\n",
            "Epoch 3 Batch 2300 Loss 1.2296 Accuracy 0.4563\n",
            "Epoch 3 Batch 2350 Loss 1.2263 Accuracy 0.4564\n",
            "Epoch 3 Batch 2400 Loss 1.2232 Accuracy 0.4568\n",
            "Epoch 3 Batch 2450 Loss 1.2196 Accuracy 0.4572\n",
            "Epoch 3 Batch 2500 Loss 1.2159 Accuracy 0.4577\n",
            "Epoch 3 Batch 2550 Loss 1.2124 Accuracy 0.4580\n",
            "Epoch 3 Batch 2600 Loss 1.2091 Accuracy 0.4583\n",
            "Epoch 3 Batch 2650 Loss 1.2060 Accuracy 0.4588\n",
            "Epoch 3 Batch 2700 Loss 1.2027 Accuracy 0.4593\n",
            "Epoch 3 Batch 2750 Loss 1.2002 Accuracy 0.4597\n",
            "Epoch 3 Batch 2800 Loss 1.1974 Accuracy 0.4601\n",
            "Epoch 3 Batch 2850 Loss 1.1947 Accuracy 0.4604\n",
            "Epoch 3 Batch 2900 Loss 1.1927 Accuracy 0.4609\n",
            "Epoch 3 Batch 2950 Loss 1.1905 Accuracy 0.4612\n",
            "Epoch 3 Batch 3000 Loss 1.1881 Accuracy 0.4616\n",
            "Epoch 3 Batch 3050 Loss 1.1855 Accuracy 0.4619\n",
            "Epoch 3 Batch 3100 Loss 1.1832 Accuracy 0.4623\n",
            "Epoch 3 Batch 3150 Loss 1.1807 Accuracy 0.4626\n",
            "Epoch 3 Batch 3200 Loss 1.1779 Accuracy 0.4627\n",
            "Epoch 3 Batch 3250 Loss 1.1750 Accuracy 0.4630\n",
            "Epoch 3 Batch 3300 Loss 1.1724 Accuracy 0.4633\n",
            "Epoch 3 Batch 3350 Loss 1.1696 Accuracy 0.4636\n",
            "Epoch 3 Batch 3400 Loss 1.1675 Accuracy 0.4639\n",
            "Epoch 3 Batch 3450 Loss 1.1653 Accuracy 0.4643\n",
            "Epoch 3 Batch 3500 Loss 1.1629 Accuracy 0.4646\n",
            "Epoch 3 Batch 3550 Loss 1.1606 Accuracy 0.4650\n",
            "Epoch 3 Batch 3600 Loss 1.1586 Accuracy 0.4654\n",
            "Epoch 3 Batch 3650 Loss 1.1563 Accuracy 0.4658\n",
            "Epoch 3 Batch 3700 Loss 1.1546 Accuracy 0.4662\n",
            "Epoch 3 Batch 3750 Loss 1.1523 Accuracy 0.4666\n",
            "Epoch 3 Batch 3800 Loss 1.1504 Accuracy 0.4671\n",
            "Epoch 3 Batch 3850 Loss 1.1487 Accuracy 0.4675\n",
            "Epoch 3 Batch 3900 Loss 1.1468 Accuracy 0.4679\n",
            "Epoch 3 Batch 3950 Loss 1.1448 Accuracy 0.4683\n",
            "Epoch 3 Batch 4000 Loss 1.1431 Accuracy 0.4687\n",
            "Epoch 3 Batch 4050 Loss 1.1414 Accuracy 0.4691\n",
            "Epoch 3 Batch 4100 Loss 1.1401 Accuracy 0.4693\n",
            "Epoch 3 Batch 4150 Loss 1.1393 Accuracy 0.4695\n",
            "Epoch 3 Batch 4200 Loss 1.1391 Accuracy 0.4696\n",
            "Epoch 3 Batch 4250 Loss 1.1390 Accuracy 0.4697\n",
            "Epoch 3 Batch 4300 Loss 1.1389 Accuracy 0.4696\n",
            "Epoch 3 Batch 4350 Loss 1.1394 Accuracy 0.4696\n",
            "Epoch 3 Batch 4400 Loss 1.1400 Accuracy 0.4695\n",
            "Epoch 3 Batch 4450 Loss 1.1408 Accuracy 0.4694\n",
            "Epoch 3 Batch 4500 Loss 1.1418 Accuracy 0.4693\n",
            "Epoch 3 Batch 4550 Loss 1.1428 Accuracy 0.4692\n",
            "Epoch 3 Batch 4600 Loss 1.1438 Accuracy 0.4690\n",
            "Epoch 3 Batch 4650 Loss 1.1448 Accuracy 0.4690\n",
            "Epoch 3 Batch 4700 Loss 1.1455 Accuracy 0.4688\n",
            "Epoch 3 Batch 4750 Loss 1.1465 Accuracy 0.4687\n",
            "Epoch 3 Batch 4800 Loss 1.1472 Accuracy 0.4686\n",
            "Epoch 3 Batch 4850 Loss 1.1479 Accuracy 0.4685\n",
            "Epoch 3 Batch 4900 Loss 1.1485 Accuracy 0.4683\n",
            "Epoch 3 Batch 4950 Loss 1.1495 Accuracy 0.4682\n",
            "Epoch 3 Batch 5000 Loss 1.1507 Accuracy 0.4680\n",
            "Epoch 3 Batch 5050 Loss 1.1517 Accuracy 0.4679\n",
            "Epoch 3 Batch 5100 Loss 1.1526 Accuracy 0.4677\n",
            "Epoch 3 Batch 5150 Loss 1.1535 Accuracy 0.4675\n",
            "Epoch 3 Batch 5200 Loss 1.1543 Accuracy 0.4674\n",
            "Epoch 3 Batch 5250 Loss 1.1549 Accuracy 0.4672\n",
            "Epoch 3 Batch 5300 Loss 1.1558 Accuracy 0.4669\n",
            "Epoch 3 Batch 5350 Loss 1.1567 Accuracy 0.4667\n",
            "Epoch 3 Batch 5400 Loss 1.1575 Accuracy 0.4665\n",
            "Epoch 3 Batch 5450 Loss 1.1582 Accuracy 0.4663\n",
            "Epoch 3 Batch 5500 Loss 1.1587 Accuracy 0.4661\n",
            "Epoch 3 Batch 5550 Loss 1.1592 Accuracy 0.4659\n",
            "Epoch 3 Batch 5600 Loss 1.1599 Accuracy 0.4658\n",
            "Epoch 3 Batch 5650 Loss 1.1602 Accuracy 0.4656\n",
            "Epoch 3 Batch 5700 Loss 1.1606 Accuracy 0.4655\n",
            "Saving checkpoint for epoch 3 at /content/drive/My Drive/Transformer/cktps/ckpt-3\n",
            "Time taken for 1 epoch: 1430.1904633045197 secs\n",
            "\n",
            "Start of epoch 4\n",
            "Epoch 4 Batch 0 Loss 1.2447 Accuracy 0.4597\n",
            "Epoch 4 Batch 50 Loss 1.2197 Accuracy 0.4526\n",
            "Epoch 4 Batch 100 Loss 1.2113 Accuracy 0.4556\n",
            "Epoch 4 Batch 150 Loss 1.2126 Accuracy 0.4561\n",
            "Epoch 4 Batch 200 Loss 1.2121 Accuracy 0.4569\n",
            "Epoch 4 Batch 250 Loss 1.2148 Accuracy 0.4575\n",
            "Epoch 4 Batch 300 Loss 1.2106 Accuracy 0.4568\n",
            "Epoch 4 Batch 350 Loss 1.2070 Accuracy 0.4573\n",
            "Epoch 4 Batch 400 Loss 1.2055 Accuracy 0.4568\n",
            "Epoch 4 Batch 450 Loss 1.2043 Accuracy 0.4567\n",
            "Epoch 4 Batch 500 Loss 1.2029 Accuracy 0.4563\n",
            "Epoch 4 Batch 550 Loss 1.2021 Accuracy 0.4567\n",
            "Epoch 4 Batch 600 Loss 1.2011 Accuracy 0.4564\n",
            "Epoch 4 Batch 650 Loss 1.2005 Accuracy 0.4568\n",
            "Epoch 4 Batch 700 Loss 1.1985 Accuracy 0.4568\n",
            "Epoch 4 Batch 750 Loss 1.1957 Accuracy 0.4572\n",
            "Epoch 4 Batch 800 Loss 1.1947 Accuracy 0.4574\n",
            "Epoch 4 Batch 850 Loss 1.1935 Accuracy 0.4581\n",
            "Epoch 4 Batch 900 Loss 1.1920 Accuracy 0.4584\n",
            "Epoch 4 Batch 950 Loss 1.1904 Accuracy 0.4584\n",
            "Epoch 4 Batch 1000 Loss 1.1884 Accuracy 0.4585\n",
            "Epoch 4 Batch 1050 Loss 1.1873 Accuracy 0.4587\n",
            "Epoch 4 Batch 1100 Loss 1.1860 Accuracy 0.4587\n",
            "Epoch 4 Batch 1150 Loss 1.1849 Accuracy 0.4588\n",
            "Epoch 4 Batch 1200 Loss 1.1843 Accuracy 0.4591\n",
            "Epoch 4 Batch 1250 Loss 1.1827 Accuracy 0.4592\n",
            "Epoch 4 Batch 1300 Loss 1.1804 Accuracy 0.4596\n",
            "Epoch 4 Batch 1350 Loss 1.1781 Accuracy 0.4604\n",
            "Epoch 4 Batch 1400 Loss 1.1756 Accuracy 0.4609\n",
            "Epoch 4 Batch 1450 Loss 1.1730 Accuracy 0.4616\n",
            "Epoch 4 Batch 1500 Loss 1.1705 Accuracy 0.4625\n",
            "Epoch 4 Batch 1550 Loss 1.1681 Accuracy 0.4633\n",
            "Epoch 4 Batch 1600 Loss 1.1655 Accuracy 0.4640\n",
            "Epoch 4 Batch 1650 Loss 1.1629 Accuracy 0.4648\n",
            "Epoch 4 Batch 1700 Loss 1.1600 Accuracy 0.4654\n",
            "Epoch 4 Batch 1750 Loss 1.1573 Accuracy 0.4664\n",
            "Epoch 4 Batch 1800 Loss 1.1550 Accuracy 0.4673\n",
            "Epoch 4 Batch 1850 Loss 1.1527 Accuracy 0.4681\n",
            "Epoch 4 Batch 1900 Loss 1.1501 Accuracy 0.4689\n",
            "Epoch 4 Batch 1950 Loss 1.1473 Accuracy 0.4696\n",
            "Epoch 4 Batch 2000 Loss 1.1453 Accuracy 0.4702\n",
            "Epoch 4 Batch 2050 Loss 1.1427 Accuracy 0.4707\n",
            "Epoch 4 Batch 2100 Loss 1.1404 Accuracy 0.4710\n",
            "Epoch 4 Batch 2150 Loss 1.1377 Accuracy 0.4712\n",
            "Epoch 4 Batch 2200 Loss 1.1339 Accuracy 0.4715\n",
            "Epoch 4 Batch 2250 Loss 1.1305 Accuracy 0.4718\n",
            "Epoch 4 Batch 2300 Loss 1.1274 Accuracy 0.4720\n",
            "Epoch 4 Batch 2350 Loss 1.1240 Accuracy 0.4723\n",
            "Epoch 4 Batch 2400 Loss 1.1207 Accuracy 0.4726\n",
            "Epoch 4 Batch 2450 Loss 1.1174 Accuracy 0.4729\n",
            "Epoch 4 Batch 2500 Loss 1.1146 Accuracy 0.4731\n",
            "Epoch 4 Batch 2550 Loss 1.1117 Accuracy 0.4736\n",
            "Epoch 4 Batch 2600 Loss 1.1089 Accuracy 0.4740\n",
            "Epoch 4 Batch 2650 Loss 1.1056 Accuracy 0.4744\n",
            "Epoch 4 Batch 2700 Loss 1.1030 Accuracy 0.4746\n",
            "Epoch 4 Batch 2750 Loss 1.1005 Accuracy 0.4750\n",
            "Epoch 4 Batch 2800 Loss 1.0983 Accuracy 0.4754\n",
            "Epoch 4 Batch 2850 Loss 1.0962 Accuracy 0.4758\n",
            "Epoch 4 Batch 2900 Loss 1.0937 Accuracy 0.4761\n",
            "Epoch 4 Batch 2950 Loss 1.0915 Accuracy 0.4764\n",
            "Epoch 4 Batch 3000 Loss 1.0897 Accuracy 0.4767\n",
            "Epoch 4 Batch 3050 Loss 1.0872 Accuracy 0.4769\n",
            "Epoch 4 Batch 3100 Loss 1.0850 Accuracy 0.4772\n",
            "Epoch 4 Batch 3150 Loss 1.0829 Accuracy 0.4775\n",
            "Epoch 4 Batch 3200 Loss 1.0809 Accuracy 0.4778\n",
            "Epoch 4 Batch 3250 Loss 1.0788 Accuracy 0.4781\n",
            "Epoch 4 Batch 3300 Loss 1.0765 Accuracy 0.4784\n",
            "Epoch 4 Batch 3350 Loss 1.0741 Accuracy 0.4788\n",
            "Epoch 4 Batch 3400 Loss 1.0721 Accuracy 0.4790\n",
            "Epoch 4 Batch 3450 Loss 1.0699 Accuracy 0.4793\n",
            "Epoch 4 Batch 3500 Loss 1.0679 Accuracy 0.4796\n",
            "Epoch 4 Batch 3550 Loss 1.0660 Accuracy 0.4800\n",
            "Epoch 4 Batch 3600 Loss 1.0639 Accuracy 0.4804\n",
            "Epoch 4 Batch 3650 Loss 1.0615 Accuracy 0.4808\n",
            "Epoch 4 Batch 3700 Loss 1.0597 Accuracy 0.4812\n",
            "Epoch 4 Batch 3750 Loss 1.0580 Accuracy 0.4815\n",
            "Epoch 4 Batch 3800 Loss 1.0562 Accuracy 0.4818\n",
            "Epoch 4 Batch 3850 Loss 1.0548 Accuracy 0.4822\n",
            "Epoch 4 Batch 3900 Loss 1.0529 Accuracy 0.4825\n",
            "Epoch 4 Batch 3950 Loss 1.0512 Accuracy 0.4829\n",
            "Epoch 4 Batch 4000 Loss 1.0498 Accuracy 0.4832\n",
            "Epoch 4 Batch 4050 Loss 1.0486 Accuracy 0.4836\n",
            "Epoch 4 Batch 4100 Loss 1.0472 Accuracy 0.4838\n",
            "Epoch 4 Batch 4150 Loss 1.0465 Accuracy 0.4839\n",
            "Epoch 4 Batch 4200 Loss 1.0469 Accuracy 0.4839\n",
            "Epoch 4 Batch 4250 Loss 1.0476 Accuracy 0.4839\n",
            "Epoch 4 Batch 4300 Loss 1.0482 Accuracy 0.4839\n",
            "Epoch 4 Batch 4350 Loss 1.0489 Accuracy 0.4838\n",
            "Epoch 4 Batch 4400 Loss 1.0501 Accuracy 0.4837\n",
            "Epoch 4 Batch 4450 Loss 1.0513 Accuracy 0.4836\n",
            "Epoch 4 Batch 4500 Loss 1.0523 Accuracy 0.4834\n",
            "Epoch 4 Batch 4550 Loss 1.0532 Accuracy 0.4833\n",
            "Epoch 4 Batch 4600 Loss 1.0546 Accuracy 0.4831\n",
            "Epoch 4 Batch 4650 Loss 1.0558 Accuracy 0.4830\n",
            "Epoch 4 Batch 4700 Loss 1.0567 Accuracy 0.4828\n",
            "Epoch 4 Batch 4750 Loss 1.0577 Accuracy 0.4827\n",
            "Epoch 4 Batch 4800 Loss 1.0588 Accuracy 0.4826\n",
            "Epoch 4 Batch 4850 Loss 1.0600 Accuracy 0.4824\n",
            "Epoch 4 Batch 4900 Loss 1.0611 Accuracy 0.4822\n",
            "Epoch 4 Batch 4950 Loss 1.0624 Accuracy 0.4820\n",
            "Epoch 4 Batch 5000 Loss 1.0636 Accuracy 0.4819\n",
            "Epoch 4 Batch 5050 Loss 1.0645 Accuracy 0.4817\n",
            "Epoch 4 Batch 5100 Loss 1.0658 Accuracy 0.4815\n",
            "Epoch 4 Batch 5150 Loss 1.0669 Accuracy 0.4813\n",
            "Epoch 4 Batch 5200 Loss 1.0681 Accuracy 0.4811\n",
            "Epoch 4 Batch 5250 Loss 1.0690 Accuracy 0.4809\n",
            "Epoch 4 Batch 5300 Loss 1.0698 Accuracy 0.4806\n",
            "Epoch 4 Batch 5350 Loss 1.0708 Accuracy 0.4804\n",
            "Epoch 4 Batch 5400 Loss 1.0714 Accuracy 0.4801\n",
            "Epoch 4 Batch 5450 Loss 1.0720 Accuracy 0.4799\n",
            "Epoch 4 Batch 5500 Loss 1.0730 Accuracy 0.4797\n",
            "Epoch 4 Batch 5550 Loss 1.0737 Accuracy 0.4795\n",
            "Epoch 4 Batch 5600 Loss 1.0745 Accuracy 0.4792\n",
            "Epoch 4 Batch 5650 Loss 1.0753 Accuracy 0.4790\n",
            "Epoch 4 Batch 5700 Loss 1.0759 Accuracy 0.4788\n",
            "Saving checkpoint for epoch 4 at /content/drive/My Drive/Transformer/cktps/ckpt-4\n",
            "Time taken for 1 epoch: 1424.835833311081 secs\n",
            "\n",
            "Start of epoch 5\n",
            "Epoch 5 Batch 0 Loss 1.0893 Accuracy 0.4663\n",
            "Epoch 5 Batch 50 Loss 1.1684 Accuracy 0.4616\n",
            "Epoch 5 Batch 100 Loss 1.1557 Accuracy 0.4643\n",
            "Epoch 5 Batch 150 Loss 1.1621 Accuracy 0.4639\n",
            "Epoch 5 Batch 200 Loss 1.1596 Accuracy 0.4653\n",
            "Epoch 5 Batch 250 Loss 1.1616 Accuracy 0.4657\n",
            "Epoch 5 Batch 300 Loss 1.1613 Accuracy 0.4653\n",
            "Epoch 5 Batch 350 Loss 1.1534 Accuracy 0.4660\n",
            "Epoch 5 Batch 400 Loss 1.1500 Accuracy 0.4669\n",
            "Epoch 5 Batch 450 Loss 1.1457 Accuracy 0.4666\n",
            "Epoch 5 Batch 500 Loss 1.1422 Accuracy 0.4666\n",
            "Epoch 5 Batch 550 Loss 1.1394 Accuracy 0.4665\n",
            "Epoch 5 Batch 600 Loss 1.1373 Accuracy 0.4665\n",
            "Epoch 5 Batch 650 Loss 1.1370 Accuracy 0.4662\n",
            "Epoch 5 Batch 700 Loss 1.1362 Accuracy 0.4669\n",
            "Epoch 5 Batch 750 Loss 1.1348 Accuracy 0.4671\n",
            "Epoch 5 Batch 800 Loss 1.1329 Accuracy 0.4673\n",
            "Epoch 5 Batch 850 Loss 1.1330 Accuracy 0.4676\n",
            "Epoch 5 Batch 900 Loss 1.1327 Accuracy 0.4677\n",
            "Epoch 5 Batch 950 Loss 1.1320 Accuracy 0.4679\n",
            "Epoch 5 Batch 1000 Loss 1.1313 Accuracy 0.4680\n",
            "Epoch 5 Batch 1050 Loss 1.1290 Accuracy 0.4680\n",
            "Epoch 5 Batch 1100 Loss 1.1276 Accuracy 0.4680\n",
            "Epoch 5 Batch 1150 Loss 1.1268 Accuracy 0.4683\n",
            "Epoch 5 Batch 1200 Loss 1.1242 Accuracy 0.4686\n",
            "Epoch 5 Batch 1250 Loss 1.1223 Accuracy 0.4690\n",
            "Epoch 5 Batch 1300 Loss 1.1201 Accuracy 0.4693\n",
            "Epoch 5 Batch 1350 Loss 1.1182 Accuracy 0.4700\n",
            "Epoch 5 Batch 1400 Loss 1.1158 Accuracy 0.4706\n",
            "Epoch 5 Batch 1450 Loss 1.1133 Accuracy 0.4713\n",
            "Epoch 5 Batch 1500 Loss 1.1107 Accuracy 0.4720\n",
            "Epoch 5 Batch 1550 Loss 1.1077 Accuracy 0.4731\n",
            "Epoch 5 Batch 1600 Loss 1.1055 Accuracy 0.4738\n",
            "Epoch 5 Batch 1650 Loss 1.1028 Accuracy 0.4747\n",
            "Epoch 5 Batch 1700 Loss 1.1000 Accuracy 0.4754\n",
            "Epoch 5 Batch 1750 Loss 1.0975 Accuracy 0.4762\n",
            "Epoch 5 Batch 1800 Loss 1.0947 Accuracy 0.4770\n",
            "Epoch 5 Batch 1850 Loss 1.0921 Accuracy 0.4778\n",
            "Epoch 5 Batch 1900 Loss 1.0897 Accuracy 0.4787\n",
            "Epoch 5 Batch 1950 Loss 1.0878 Accuracy 0.4795\n",
            "Epoch 5 Batch 2000 Loss 1.0864 Accuracy 0.4801\n",
            "Epoch 5 Batch 2050 Loss 1.0836 Accuracy 0.4806\n",
            "Epoch 5 Batch 2100 Loss 1.0817 Accuracy 0.4809\n",
            "Epoch 5 Batch 2150 Loss 1.0787 Accuracy 0.4812\n",
            "Epoch 5 Batch 2200 Loss 1.0753 Accuracy 0.4815\n",
            "Epoch 5 Batch 2250 Loss 1.0727 Accuracy 0.4816\n",
            "Epoch 5 Batch 2300 Loss 1.0703 Accuracy 0.4818\n",
            "Epoch 5 Batch 2350 Loss 1.0676 Accuracy 0.4821\n",
            "Epoch 5 Batch 2400 Loss 1.0646 Accuracy 0.4823\n",
            "Epoch 5 Batch 2450 Loss 1.0615 Accuracy 0.4826\n",
            "Epoch 5 Batch 2500 Loss 1.0576 Accuracy 0.4828\n",
            "Epoch 5 Batch 2550 Loss 1.0550 Accuracy 0.4832\n",
            "Epoch 5 Batch 2600 Loss 1.0527 Accuracy 0.4834\n",
            "Epoch 5 Batch 2650 Loss 1.0499 Accuracy 0.4837\n",
            "Epoch 5 Batch 2700 Loss 1.0474 Accuracy 0.4841\n",
            "Epoch 5 Batch 2750 Loss 1.0451 Accuracy 0.4844\n",
            "Epoch 5 Batch 2800 Loss 1.0426 Accuracy 0.4847\n",
            "Epoch 5 Batch 2850 Loss 1.0405 Accuracy 0.4849\n",
            "Epoch 5 Batch 2900 Loss 1.0384 Accuracy 0.4852\n",
            "Epoch 5 Batch 2950 Loss 1.0364 Accuracy 0.4855\n",
            "Epoch 5 Batch 3000 Loss 1.0343 Accuracy 0.4858\n",
            "Epoch 5 Batch 3050 Loss 1.0322 Accuracy 0.4861\n",
            "Epoch 5 Batch 3100 Loss 1.0300 Accuracy 0.4863\n",
            "Epoch 5 Batch 3150 Loss 1.0281 Accuracy 0.4866\n",
            "Epoch 5 Batch 3200 Loss 1.0258 Accuracy 0.4869\n",
            "Epoch 5 Batch 3250 Loss 1.0234 Accuracy 0.4871\n",
            "Epoch 5 Batch 3300 Loss 1.0215 Accuracy 0.4874\n",
            "Epoch 5 Batch 3350 Loss 1.0191 Accuracy 0.4878\n",
            "Epoch 5 Batch 3400 Loss 1.0173 Accuracy 0.4881\n",
            "Epoch 5 Batch 3450 Loss 1.0155 Accuracy 0.4884\n",
            "Epoch 5 Batch 3500 Loss 1.0132 Accuracy 0.4888\n",
            "Epoch 5 Batch 3550 Loss 1.0111 Accuracy 0.4892\n",
            "Epoch 5 Batch 3600 Loss 1.0091 Accuracy 0.4895\n",
            "Epoch 5 Batch 3650 Loss 1.0075 Accuracy 0.4898\n",
            "Epoch 5 Batch 3700 Loss 1.0056 Accuracy 0.4901\n",
            "Epoch 5 Batch 3750 Loss 1.0039 Accuracy 0.4904\n",
            "Epoch 5 Batch 3800 Loss 1.0023 Accuracy 0.4907\n",
            "Epoch 5 Batch 3850 Loss 1.0009 Accuracy 0.4911\n",
            "Epoch 5 Batch 3900 Loss 0.9992 Accuracy 0.4914\n",
            "Epoch 5 Batch 3950 Loss 0.9975 Accuracy 0.4917\n",
            "Epoch 5 Batch 4000 Loss 0.9958 Accuracy 0.4921\n",
            "Epoch 5 Batch 4050 Loss 0.9947 Accuracy 0.4924\n",
            "Epoch 5 Batch 4100 Loss 0.9936 Accuracy 0.4925\n",
            "Epoch 5 Batch 4150 Loss 0.9930 Accuracy 0.4927\n",
            "Epoch 5 Batch 4200 Loss 0.9933 Accuracy 0.4927\n",
            "Epoch 5 Batch 4250 Loss 0.9935 Accuracy 0.4927\n",
            "Epoch 5 Batch 4300 Loss 0.9940 Accuracy 0.4926\n",
            "Epoch 5 Batch 4350 Loss 0.9950 Accuracy 0.4925\n",
            "Epoch 5 Batch 4400 Loss 0.9957 Accuracy 0.4924\n",
            "Epoch 5 Batch 4450 Loss 0.9971 Accuracy 0.4922\n",
            "Epoch 5 Batch 4500 Loss 0.9983 Accuracy 0.4920\n",
            "Epoch 5 Batch 4550 Loss 0.9994 Accuracy 0.4919\n",
            "Epoch 5 Batch 4600 Loss 1.0009 Accuracy 0.4917\n",
            "Epoch 5 Batch 4650 Loss 1.0023 Accuracy 0.4916\n",
            "Epoch 5 Batch 4700 Loss 1.0036 Accuracy 0.4914\n",
            "Epoch 5 Batch 4750 Loss 1.0049 Accuracy 0.4912\n",
            "Epoch 5 Batch 4800 Loss 1.0061 Accuracy 0.4910\n",
            "Epoch 5 Batch 4850 Loss 1.0071 Accuracy 0.4909\n",
            "Epoch 5 Batch 4900 Loss 1.0081 Accuracy 0.4907\n",
            "Epoch 5 Batch 4950 Loss 1.0093 Accuracy 0.4905\n",
            "Epoch 5 Batch 5000 Loss 1.0107 Accuracy 0.4903\n",
            "Epoch 5 Batch 5050 Loss 1.0118 Accuracy 0.4901\n",
            "Epoch 5 Batch 5100 Loss 1.0127 Accuracy 0.4899\n",
            "Epoch 5 Batch 5150 Loss 1.0141 Accuracy 0.4897\n",
            "Epoch 5 Batch 5200 Loss 1.0153 Accuracy 0.4895\n",
            "Epoch 5 Batch 5250 Loss 1.0167 Accuracy 0.4892\n",
            "Epoch 5 Batch 5300 Loss 1.0178 Accuracy 0.4889\n",
            "Epoch 5 Batch 5350 Loss 1.0186 Accuracy 0.4887\n",
            "Epoch 5 Batch 5400 Loss 1.0198 Accuracy 0.4884\n",
            "Epoch 5 Batch 5450 Loss 1.0207 Accuracy 0.4881\n",
            "Epoch 5 Batch 5500 Loss 1.0218 Accuracy 0.4879\n",
            "Epoch 5 Batch 5550 Loss 1.0227 Accuracy 0.4877\n",
            "Epoch 5 Batch 5600 Loss 1.0234 Accuracy 0.4874\n",
            "Epoch 5 Batch 5650 Loss 1.0242 Accuracy 0.4872\n",
            "Epoch 5 Batch 5700 Loss 1.0248 Accuracy 0.4870\n",
            "Saving checkpoint for epoch 5 at /content/drive/My Drive/Transformer/cktps/ckpt-5\n",
            "Time taken for 1 epoch: 1431.8750441074371 secs\n",
            "\n",
            "Start of epoch 6\n",
            "Epoch 6 Batch 0 Loss 1.1056 Accuracy 0.4474\n",
            "Epoch 6 Batch 50 Loss 1.0986 Accuracy 0.4699\n",
            "Epoch 6 Batch 100 Loss 1.1141 Accuracy 0.4721\n",
            "Epoch 6 Batch 150 Loss 1.1120 Accuracy 0.4717\n",
            "Epoch 6 Batch 200 Loss 1.1089 Accuracy 0.4720\n",
            "Epoch 6 Batch 250 Loss 1.1099 Accuracy 0.4727\n",
            "Epoch 6 Batch 300 Loss 1.1071 Accuracy 0.4734\n",
            "Epoch 6 Batch 350 Loss 1.1061 Accuracy 0.4736\n",
            "Epoch 6 Batch 400 Loss 1.1086 Accuracy 0.4735\n",
            "Epoch 6 Batch 450 Loss 1.1042 Accuracy 0.4730\n",
            "Epoch 6 Batch 500 Loss 1.1028 Accuracy 0.4725\n",
            "Epoch 6 Batch 550 Loss 1.1032 Accuracy 0.4723\n",
            "Epoch 6 Batch 600 Loss 1.1007 Accuracy 0.4727\n",
            "Epoch 6 Batch 650 Loss 1.0993 Accuracy 0.4731\n",
            "Epoch 6 Batch 700 Loss 1.0972 Accuracy 0.4737\n",
            "Epoch 6 Batch 750 Loss 1.0956 Accuracy 0.4736\n",
            "Epoch 6 Batch 800 Loss 1.0930 Accuracy 0.4742\n",
            "Epoch 6 Batch 850 Loss 1.0920 Accuracy 0.4743\n",
            "Epoch 6 Batch 900 Loss 1.0928 Accuracy 0.4743\n",
            "Epoch 6 Batch 950 Loss 1.0907 Accuracy 0.4744\n",
            "Epoch 6 Batch 1000 Loss 1.0885 Accuracy 0.4745\n",
            "Epoch 6 Batch 1050 Loss 1.0881 Accuracy 0.4743\n",
            "Epoch 6 Batch 1100 Loss 1.0865 Accuracy 0.4745\n",
            "Epoch 6 Batch 1150 Loss 1.0853 Accuracy 0.4746\n",
            "Epoch 6 Batch 1200 Loss 1.0840 Accuracy 0.4748\n",
            "Epoch 6 Batch 1250 Loss 1.0819 Accuracy 0.4751\n",
            "Epoch 6 Batch 1300 Loss 1.0799 Accuracy 0.4755\n",
            "Epoch 6 Batch 1350 Loss 1.0775 Accuracy 0.4763\n",
            "Epoch 6 Batch 1400 Loss 1.0757 Accuracy 0.4768\n",
            "Epoch 6 Batch 1450 Loss 1.0730 Accuracy 0.4776\n",
            "Epoch 6 Batch 1500 Loss 1.0702 Accuracy 0.4786\n",
            "Epoch 6 Batch 1550 Loss 1.0685 Accuracy 0.4794\n",
            "Epoch 6 Batch 1600 Loss 1.0656 Accuracy 0.4802\n",
            "Epoch 6 Batch 1650 Loss 1.0627 Accuracy 0.4809\n",
            "Epoch 6 Batch 1700 Loss 1.0602 Accuracy 0.4816\n",
            "Epoch 6 Batch 1750 Loss 1.0583 Accuracy 0.4824\n",
            "Epoch 6 Batch 1800 Loss 1.0558 Accuracy 0.4832\n",
            "Epoch 6 Batch 1850 Loss 1.0537 Accuracy 0.4841\n",
            "Epoch 6 Batch 1900 Loss 1.0517 Accuracy 0.4850\n",
            "Epoch 6 Batch 1950 Loss 1.0490 Accuracy 0.4858\n",
            "Epoch 6 Batch 2000 Loss 1.0469 Accuracy 0.4864\n",
            "Epoch 6 Batch 2050 Loss 1.0444 Accuracy 0.4869\n",
            "Epoch 6 Batch 2100 Loss 1.0418 Accuracy 0.4873\n",
            "Epoch 6 Batch 2150 Loss 1.0391 Accuracy 0.4875\n",
            "Epoch 6 Batch 2200 Loss 1.0363 Accuracy 0.4876\n",
            "Epoch 6 Batch 2250 Loss 1.0332 Accuracy 0.4878\n",
            "Epoch 6 Batch 2300 Loss 1.0298 Accuracy 0.4879\n",
            "Epoch 6 Batch 2350 Loss 1.0272 Accuracy 0.4882\n",
            "Epoch 6 Batch 2400 Loss 1.0249 Accuracy 0.4884\n",
            "Epoch 6 Batch 2450 Loss 1.0222 Accuracy 0.4886\n",
            "Epoch 6 Batch 2500 Loss 1.0189 Accuracy 0.4889\n",
            "Epoch 6 Batch 2550 Loss 1.0160 Accuracy 0.4893\n",
            "Epoch 6 Batch 2600 Loss 1.0130 Accuracy 0.4896\n",
            "Epoch 6 Batch 2650 Loss 1.0103 Accuracy 0.4900\n",
            "Epoch 6 Batch 2700 Loss 1.0077 Accuracy 0.4904\n",
            "Epoch 6 Batch 2750 Loss 1.0051 Accuracy 0.4906\n",
            "Epoch 6 Batch 2800 Loss 1.0027 Accuracy 0.4908\n",
            "Epoch 6 Batch 2850 Loss 1.0004 Accuracy 0.4912\n",
            "Epoch 6 Batch 2900 Loss 0.9986 Accuracy 0.4915\n",
            "Epoch 6 Batch 2950 Loss 0.9963 Accuracy 0.4918\n",
            "Epoch 6 Batch 3000 Loss 0.9945 Accuracy 0.4921\n",
            "Epoch 6 Batch 3050 Loss 0.9928 Accuracy 0.4923\n",
            "Epoch 6 Batch 3100 Loss 0.9911 Accuracy 0.4925\n",
            "Epoch 6 Batch 3150 Loss 0.9893 Accuracy 0.4927\n",
            "Epoch 6 Batch 3200 Loss 0.9872 Accuracy 0.4929\n",
            "Epoch 6 Batch 3250 Loss 0.9848 Accuracy 0.4931\n",
            "Epoch 6 Batch 3300 Loss 0.9828 Accuracy 0.4935\n",
            "Epoch 6 Batch 3350 Loss 0.9806 Accuracy 0.4938\n",
            "Epoch 6 Batch 3400 Loss 0.9787 Accuracy 0.4940\n",
            "Epoch 6 Batch 3450 Loss 0.9771 Accuracy 0.4944\n",
            "Epoch 6 Batch 3500 Loss 0.9749 Accuracy 0.4947\n",
            "Epoch 6 Batch 3550 Loss 0.9732 Accuracy 0.4950\n",
            "Epoch 6 Batch 3600 Loss 0.9714 Accuracy 0.4953\n",
            "Epoch 6 Batch 3650 Loss 0.9695 Accuracy 0.4957\n",
            "Epoch 6 Batch 3700 Loss 0.9679 Accuracy 0.4961\n",
            "Epoch 6 Batch 3750 Loss 0.9663 Accuracy 0.4964\n",
            "Epoch 6 Batch 3800 Loss 0.9646 Accuracy 0.4968\n",
            "Epoch 6 Batch 3850 Loss 0.9634 Accuracy 0.4971\n",
            "Epoch 6 Batch 3900 Loss 0.9620 Accuracy 0.4974\n",
            "Epoch 6 Batch 3950 Loss 0.9608 Accuracy 0.4977\n",
            "Epoch 6 Batch 4000 Loss 0.9593 Accuracy 0.4980\n",
            "Epoch 6 Batch 4050 Loss 0.9583 Accuracy 0.4983\n",
            "Epoch 6 Batch 4100 Loss 0.9574 Accuracy 0.4985\n",
            "Epoch 6 Batch 4150 Loss 0.9567 Accuracy 0.4986\n",
            "Epoch 6 Batch 4200 Loss 0.9566 Accuracy 0.4987\n",
            "Epoch 6 Batch 4250 Loss 0.9568 Accuracy 0.4987\n",
            "Epoch 6 Batch 4300 Loss 0.9575 Accuracy 0.4986\n",
            "Epoch 6 Batch 4350 Loss 0.9587 Accuracy 0.4985\n",
            "Epoch 6 Batch 4400 Loss 0.9597 Accuracy 0.4983\n",
            "Epoch 6 Batch 4450 Loss 0.9608 Accuracy 0.4982\n",
            "Epoch 6 Batch 4500 Loss 0.9620 Accuracy 0.4981\n",
            "Epoch 6 Batch 4550 Loss 0.9635 Accuracy 0.4978\n",
            "Epoch 6 Batch 4600 Loss 0.9647 Accuracy 0.4976\n",
            "Epoch 6 Batch 4650 Loss 0.9663 Accuracy 0.4974\n",
            "Epoch 6 Batch 4700 Loss 0.9677 Accuracy 0.4972\n",
            "Epoch 6 Batch 4750 Loss 0.9690 Accuracy 0.4971\n",
            "Epoch 6 Batch 4800 Loss 0.9702 Accuracy 0.4969\n",
            "Epoch 6 Batch 4850 Loss 0.9712 Accuracy 0.4968\n",
            "Epoch 6 Batch 4900 Loss 0.9724 Accuracy 0.4966\n",
            "Epoch 6 Batch 4950 Loss 0.9736 Accuracy 0.4964\n",
            "Epoch 6 Batch 5000 Loss 0.9748 Accuracy 0.4962\n",
            "Epoch 6 Batch 5050 Loss 0.9760 Accuracy 0.4960\n",
            "Epoch 6 Batch 5100 Loss 0.9771 Accuracy 0.4958\n",
            "Epoch 6 Batch 5150 Loss 0.9784 Accuracy 0.4955\n",
            "Epoch 6 Batch 5200 Loss 0.9795 Accuracy 0.4953\n",
            "Epoch 6 Batch 5250 Loss 0.9807 Accuracy 0.4950\n",
            "Epoch 6 Batch 5300 Loss 0.9820 Accuracy 0.4947\n",
            "Epoch 6 Batch 5350 Loss 0.9832 Accuracy 0.4944\n",
            "Epoch 6 Batch 5400 Loss 0.9843 Accuracy 0.4942\n",
            "Epoch 6 Batch 5450 Loss 0.9854 Accuracy 0.4939\n",
            "Epoch 6 Batch 5500 Loss 0.9863 Accuracy 0.4937\n",
            "Epoch 6 Batch 5550 Loss 0.9871 Accuracy 0.4934\n",
            "Epoch 6 Batch 5600 Loss 0.9880 Accuracy 0.4932\n",
            "Epoch 6 Batch 5650 Loss 0.9888 Accuracy 0.4929\n",
            "Epoch 6 Batch 5700 Loss 0.9896 Accuracy 0.4927\n",
            "Saving checkpoint for epoch 6 at /content/drive/My Drive/Transformer/cktps/ckpt-6\n",
            "Time taken for 1 epoch: 1435.1671249866486 secs\n",
            "\n",
            "Start of epoch 7\n",
            "Epoch 7 Batch 0 Loss 0.9863 Accuracy 0.5025\n",
            "Epoch 7 Batch 50 Loss 1.0957 Accuracy 0.4767\n",
            "Epoch 7 Batch 100 Loss 1.0857 Accuracy 0.4779\n",
            "Epoch 7 Batch 150 Loss 1.0835 Accuracy 0.4769\n",
            "Epoch 7 Batch 200 Loss 1.0792 Accuracy 0.4770\n",
            "Epoch 7 Batch 250 Loss 1.0752 Accuracy 0.4763\n",
            "Epoch 7 Batch 300 Loss 1.0737 Accuracy 0.4765\n",
            "Epoch 7 Batch 350 Loss 1.0692 Accuracy 0.4765\n",
            "Epoch 7 Batch 400 Loss 1.0693 Accuracy 0.4773\n",
            "Epoch 7 Batch 450 Loss 1.0642 Accuracy 0.4773\n",
            "Epoch 7 Batch 500 Loss 1.0655 Accuracy 0.4772\n",
            "Epoch 7 Batch 550 Loss 1.0651 Accuracy 0.4772\n",
            "Epoch 7 Batch 600 Loss 1.0638 Accuracy 0.4771\n",
            "Epoch 7 Batch 650 Loss 1.0640 Accuracy 0.4770\n",
            "Epoch 7 Batch 700 Loss 1.0640 Accuracy 0.4775\n",
            "Epoch 7 Batch 750 Loss 1.0651 Accuracy 0.4778\n",
            "Epoch 7 Batch 800 Loss 1.0649 Accuracy 0.4782\n",
            "Epoch 7 Batch 850 Loss 1.0637 Accuracy 0.4782\n",
            "Epoch 7 Batch 900 Loss 1.0625 Accuracy 0.4784\n",
            "Epoch 7 Batch 950 Loss 1.0615 Accuracy 0.4784\n",
            "Epoch 7 Batch 1000 Loss 1.0596 Accuracy 0.4786\n",
            "Epoch 7 Batch 1050 Loss 1.0586 Accuracy 0.4787\n",
            "Epoch 7 Batch 1100 Loss 1.0577 Accuracy 0.4790\n",
            "Epoch 7 Batch 1150 Loss 1.0567 Accuracy 0.4791\n",
            "Epoch 7 Batch 1200 Loss 1.0548 Accuracy 0.4797\n",
            "Epoch 7 Batch 1250 Loss 1.0523 Accuracy 0.4799\n",
            "Epoch 7 Batch 1300 Loss 1.0500 Accuracy 0.4805\n",
            "Epoch 7 Batch 1350 Loss 1.0484 Accuracy 0.4809\n",
            "Epoch 7 Batch 1400 Loss 1.0469 Accuracy 0.4816\n",
            "Epoch 7 Batch 1450 Loss 1.0435 Accuracy 0.4823\n",
            "Epoch 7 Batch 1500 Loss 1.0411 Accuracy 0.4830\n",
            "Epoch 7 Batch 1550 Loss 1.0386 Accuracy 0.4837\n",
            "Epoch 7 Batch 1600 Loss 1.0364 Accuracy 0.4846\n",
            "Epoch 7 Batch 1650 Loss 1.0336 Accuracy 0.4854\n",
            "Epoch 7 Batch 1700 Loss 1.0306 Accuracy 0.4861\n",
            "Epoch 7 Batch 1750 Loss 1.0275 Accuracy 0.4870\n",
            "Epoch 7 Batch 1800 Loss 1.0253 Accuracy 0.4879\n",
            "Epoch 7 Batch 1850 Loss 1.0235 Accuracy 0.4887\n",
            "Epoch 7 Batch 1900 Loss 1.0211 Accuracy 0.4895\n",
            "Epoch 7 Batch 1950 Loss 1.0199 Accuracy 0.4903\n",
            "Epoch 7 Batch 2000 Loss 1.0174 Accuracy 0.4909\n",
            "Epoch 7 Batch 2050 Loss 1.0155 Accuracy 0.4913\n",
            "Epoch 7 Batch 2100 Loss 1.0132 Accuracy 0.4917\n",
            "Epoch 7 Batch 2150 Loss 1.0100 Accuracy 0.4920\n",
            "Epoch 7 Batch 2200 Loss 1.0075 Accuracy 0.4922\n",
            "Epoch 7 Batch 2250 Loss 1.0049 Accuracy 0.4923\n",
            "Epoch 7 Batch 2300 Loss 1.0023 Accuracy 0.4924\n",
            "Epoch 7 Batch 2350 Loss 0.9990 Accuracy 0.4927\n",
            "Epoch 7 Batch 2400 Loss 0.9965 Accuracy 0.4930\n",
            "Epoch 7 Batch 2450 Loss 0.9937 Accuracy 0.4933\n",
            "Epoch 7 Batch 2500 Loss 0.9909 Accuracy 0.4936\n",
            "Epoch 7 Batch 2550 Loss 0.9881 Accuracy 0.4938\n",
            "Epoch 7 Batch 2600 Loss 0.9852 Accuracy 0.4942\n",
            "Epoch 7 Batch 2650 Loss 0.9825 Accuracy 0.4945\n",
            "Epoch 7 Batch 2700 Loss 0.9798 Accuracy 0.4948\n",
            "Epoch 7 Batch 2750 Loss 0.9777 Accuracy 0.4951\n",
            "Epoch 7 Batch 2800 Loss 0.9758 Accuracy 0.4954\n",
            "Epoch 7 Batch 2850 Loss 0.9734 Accuracy 0.4957\n",
            "Epoch 7 Batch 2900 Loss 0.9710 Accuracy 0.4960\n",
            "Epoch 7 Batch 2950 Loss 0.9688 Accuracy 0.4963\n",
            "Epoch 7 Batch 3000 Loss 0.9670 Accuracy 0.4966\n",
            "Epoch 7 Batch 3050 Loss 0.9650 Accuracy 0.4968\n",
            "Epoch 7 Batch 3100 Loss 0.9635 Accuracy 0.4970\n",
            "Epoch 7 Batch 3150 Loss 0.9616 Accuracy 0.4973\n",
            "Epoch 7 Batch 3200 Loss 0.9597 Accuracy 0.4976\n",
            "Epoch 7 Batch 3250 Loss 0.9578 Accuracy 0.4978\n",
            "Epoch 7 Batch 3300 Loss 0.9557 Accuracy 0.4980\n",
            "Epoch 7 Batch 3350 Loss 0.9536 Accuracy 0.4983\n",
            "Epoch 7 Batch 3400 Loss 0.9516 Accuracy 0.4986\n",
            "Epoch 7 Batch 3450 Loss 0.9497 Accuracy 0.4989\n",
            "Epoch 7 Batch 3500 Loss 0.9478 Accuracy 0.4993\n",
            "Epoch 7 Batch 3550 Loss 0.9462 Accuracy 0.4996\n",
            "Epoch 7 Batch 3600 Loss 0.9444 Accuracy 0.4999\n",
            "Epoch 7 Batch 3650 Loss 0.9426 Accuracy 0.5003\n",
            "Epoch 7 Batch 3700 Loss 0.9411 Accuracy 0.5006\n",
            "Epoch 7 Batch 3750 Loss 0.9394 Accuracy 0.5009\n",
            "Epoch 7 Batch 3800 Loss 0.9382 Accuracy 0.5013\n",
            "Epoch 7 Batch 3850 Loss 0.9366 Accuracy 0.5016\n",
            "Epoch 7 Batch 3900 Loss 0.9351 Accuracy 0.5019\n",
            "Epoch 7 Batch 3950 Loss 0.9337 Accuracy 0.5021\n",
            "Epoch 7 Batch 4000 Loss 0.9325 Accuracy 0.5025\n",
            "Epoch 7 Batch 4050 Loss 0.9314 Accuracy 0.5029\n",
            "Epoch 7 Batch 4100 Loss 0.9303 Accuracy 0.5031\n",
            "Epoch 7 Batch 4150 Loss 0.9299 Accuracy 0.5031\n",
            "Epoch 7 Batch 4200 Loss 0.9298 Accuracy 0.5031\n",
            "Epoch 7 Batch 4250 Loss 0.9304 Accuracy 0.5031\n",
            "Epoch 7 Batch 4300 Loss 0.9311 Accuracy 0.5030\n",
            "Epoch 7 Batch 4350 Loss 0.9322 Accuracy 0.5029\n",
            "Epoch 7 Batch 4400 Loss 0.9331 Accuracy 0.5027\n",
            "Epoch 7 Batch 4450 Loss 0.9341 Accuracy 0.5026\n",
            "Epoch 7 Batch 4500 Loss 0.9356 Accuracy 0.5024\n",
            "Epoch 7 Batch 4550 Loss 0.9372 Accuracy 0.5022\n",
            "Epoch 7 Batch 4600 Loss 0.9385 Accuracy 0.5020\n",
            "Epoch 7 Batch 4650 Loss 0.9401 Accuracy 0.5018\n",
            "Epoch 7 Batch 4700 Loss 0.9415 Accuracy 0.5016\n",
            "Epoch 7 Batch 4750 Loss 0.9425 Accuracy 0.5014\n",
            "Epoch 7 Batch 4800 Loss 0.9435 Accuracy 0.5012\n",
            "Epoch 7 Batch 4850 Loss 0.9445 Accuracy 0.5010\n",
            "Epoch 7 Batch 4900 Loss 0.9461 Accuracy 0.5008\n",
            "Epoch 7 Batch 4950 Loss 0.9474 Accuracy 0.5006\n",
            "Epoch 7 Batch 5000 Loss 0.9487 Accuracy 0.5004\n",
            "Epoch 7 Batch 5050 Loss 0.9500 Accuracy 0.5002\n",
            "Epoch 7 Batch 5100 Loss 0.9513 Accuracy 0.5000\n",
            "Epoch 7 Batch 5150 Loss 0.9526 Accuracy 0.4998\n",
            "Epoch 7 Batch 5200 Loss 0.9538 Accuracy 0.4996\n",
            "Epoch 7 Batch 5250 Loss 0.9548 Accuracy 0.4993\n",
            "Epoch 7 Batch 5300 Loss 0.9560 Accuracy 0.4990\n",
            "Epoch 7 Batch 5350 Loss 0.9572 Accuracy 0.4987\n",
            "Epoch 7 Batch 5400 Loss 0.9580 Accuracy 0.4984\n",
            "Epoch 7 Batch 5450 Loss 0.9589 Accuracy 0.4981\n",
            "Epoch 7 Batch 5500 Loss 0.9599 Accuracy 0.4979\n",
            "Epoch 7 Batch 5550 Loss 0.9610 Accuracy 0.4976\n",
            "Epoch 7 Batch 5600 Loss 0.9621 Accuracy 0.4974\n",
            "Epoch 7 Batch 5650 Loss 0.9629 Accuracy 0.4972\n",
            "Epoch 7 Batch 5700 Loss 0.9637 Accuracy 0.4970\n",
            "Saving checkpoint for epoch 7 at /content/drive/My Drive/Transformer/cktps/ckpt-7\n",
            "Time taken for 1 epoch: 1429.1744101047516 secs\n",
            "\n",
            "Start of epoch 8\n",
            "Epoch 8 Batch 0 Loss 1.0335 Accuracy 0.4819\n",
            "Epoch 8 Batch 50 Loss 1.0641 Accuracy 0.4827\n",
            "Epoch 8 Batch 100 Loss 1.0483 Accuracy 0.4837\n",
            "Epoch 8 Batch 150 Loss 1.0518 Accuracy 0.4829\n",
            "Epoch 8 Batch 200 Loss 1.0529 Accuracy 0.4833\n",
            "Epoch 8 Batch 250 Loss 1.0496 Accuracy 0.4826\n",
            "Epoch 8 Batch 300 Loss 1.0447 Accuracy 0.4827\n",
            "Epoch 8 Batch 350 Loss 1.0506 Accuracy 0.4816\n",
            "Epoch 8 Batch 400 Loss 1.0462 Accuracy 0.4813\n",
            "Epoch 8 Batch 450 Loss 1.0449 Accuracy 0.4818\n",
            "Epoch 8 Batch 500 Loss 1.0438 Accuracy 0.4810\n",
            "Epoch 8 Batch 550 Loss 1.0449 Accuracy 0.4814\n",
            "Epoch 8 Batch 600 Loss 1.0421 Accuracy 0.4811\n",
            "Epoch 8 Batch 650 Loss 1.0423 Accuracy 0.4814\n",
            "Epoch 8 Batch 700 Loss 1.0435 Accuracy 0.4819\n",
            "Epoch 8 Batch 750 Loss 1.0411 Accuracy 0.4827\n",
            "Epoch 8 Batch 800 Loss 1.0410 Accuracy 0.4829\n",
            "Epoch 8 Batch 850 Loss 1.0407 Accuracy 0.4827\n",
            "Epoch 8 Batch 900 Loss 1.0404 Accuracy 0.4827\n",
            "Epoch 8 Batch 950 Loss 1.0396 Accuracy 0.4827\n",
            "Epoch 8 Batch 1000 Loss 1.0380 Accuracy 0.4826\n",
            "Epoch 8 Batch 1050 Loss 1.0367 Accuracy 0.4826\n",
            "Epoch 8 Batch 1100 Loss 1.0335 Accuracy 0.4829\n",
            "Epoch 8 Batch 1150 Loss 1.0314 Accuracy 0.4832\n",
            "Epoch 8 Batch 1200 Loss 1.0294 Accuracy 0.4835\n",
            "Epoch 8 Batch 1250 Loss 1.0276 Accuracy 0.4838\n",
            "Epoch 8 Batch 1300 Loss 1.0261 Accuracy 0.4843\n",
            "Epoch 8 Batch 1350 Loss 1.0246 Accuracy 0.4849\n",
            "Epoch 8 Batch 1400 Loss 1.0218 Accuracy 0.4858\n",
            "Epoch 8 Batch 1450 Loss 1.0194 Accuracy 0.4863\n",
            "Epoch 8 Batch 1500 Loss 1.0172 Accuracy 0.4872\n",
            "Epoch 8 Batch 1550 Loss 1.0146 Accuracy 0.4880\n",
            "Epoch 8 Batch 1600 Loss 1.0121 Accuracy 0.4888\n",
            "Epoch 8 Batch 1650 Loss 1.0101 Accuracy 0.4897\n",
            "Epoch 8 Batch 1700 Loss 1.0078 Accuracy 0.4904\n",
            "Epoch 8 Batch 1750 Loss 1.0052 Accuracy 0.4910\n",
            "Epoch 8 Batch 1800 Loss 1.0033 Accuracy 0.4917\n",
            "Epoch 8 Batch 1850 Loss 1.0008 Accuracy 0.4926\n",
            "Epoch 8 Batch 1900 Loss 0.9982 Accuracy 0.4933\n",
            "Epoch 8 Batch 1950 Loss 0.9961 Accuracy 0.4940\n",
            "Epoch 8 Batch 2000 Loss 0.9946 Accuracy 0.4946\n",
            "Epoch 8 Batch 2050 Loss 0.9925 Accuracy 0.4950\n",
            "Epoch 8 Batch 2100 Loss 0.9902 Accuracy 0.4955\n",
            "Epoch 8 Batch 2150 Loss 0.9876 Accuracy 0.4958\n",
            "Epoch 8 Batch 2200 Loss 0.9845 Accuracy 0.4960\n",
            "Epoch 8 Batch 2250 Loss 0.9819 Accuracy 0.4963\n",
            "Epoch 8 Batch 2300 Loss 0.9790 Accuracy 0.4964\n",
            "Epoch 8 Batch 2350 Loss 0.9761 Accuracy 0.4966\n",
            "Epoch 8 Batch 2400 Loss 0.9735 Accuracy 0.4968\n",
            "Epoch 8 Batch 2450 Loss 0.9705 Accuracy 0.4970\n",
            "Epoch 8 Batch 2500 Loss 0.9679 Accuracy 0.4973\n",
            "Epoch 8 Batch 2550 Loss 0.9651 Accuracy 0.4976\n",
            "Epoch 8 Batch 2600 Loss 0.9624 Accuracy 0.4980\n",
            "Epoch 8 Batch 2650 Loss 0.9598 Accuracy 0.4982\n",
            "Epoch 8 Batch 2700 Loss 0.9575 Accuracy 0.4984\n",
            "Epoch 8 Batch 2750 Loss 0.9556 Accuracy 0.4987\n",
            "Epoch 8 Batch 2800 Loss 0.9536 Accuracy 0.4990\n",
            "Epoch 8 Batch 2850 Loss 0.9510 Accuracy 0.4993\n",
            "Epoch 8 Batch 2900 Loss 0.9491 Accuracy 0.4996\n",
            "Epoch 8 Batch 2950 Loss 0.9471 Accuracy 0.4999\n",
            "Epoch 8 Batch 3000 Loss 0.9451 Accuracy 0.5001\n",
            "Epoch 8 Batch 3050 Loss 0.9431 Accuracy 0.5004\n",
            "Epoch 8 Batch 3100 Loss 0.9413 Accuracy 0.5007\n",
            "Epoch 8 Batch 3150 Loss 0.9394 Accuracy 0.5009\n",
            "Epoch 8 Batch 3200 Loss 0.9374 Accuracy 0.5011\n",
            "Epoch 8 Batch 3250 Loss 0.9355 Accuracy 0.5013\n",
            "Epoch 8 Batch 3300 Loss 0.9338 Accuracy 0.5016\n",
            "Epoch 8 Batch 3350 Loss 0.9320 Accuracy 0.5019\n",
            "Epoch 8 Batch 3400 Loss 0.9302 Accuracy 0.5022\n",
            "Epoch 8 Batch 3450 Loss 0.9284 Accuracy 0.5026\n",
            "Epoch 8 Batch 3500 Loss 0.9267 Accuracy 0.5029\n",
            "Epoch 8 Batch 3550 Loss 0.9249 Accuracy 0.5032\n",
            "Epoch 8 Batch 3600 Loss 0.9235 Accuracy 0.5035\n",
            "Epoch 8 Batch 3650 Loss 0.9213 Accuracy 0.5038\n",
            "Epoch 8 Batch 3700 Loss 0.9197 Accuracy 0.5042\n",
            "Epoch 8 Batch 3750 Loss 0.9180 Accuracy 0.5044\n",
            "Epoch 8 Batch 3800 Loss 0.9168 Accuracy 0.5048\n",
            "Epoch 8 Batch 3850 Loss 0.9154 Accuracy 0.5051\n",
            "Epoch 8 Batch 3900 Loss 0.9140 Accuracy 0.5055\n",
            "Epoch 8 Batch 3950 Loss 0.9126 Accuracy 0.5058\n",
            "Epoch 8 Batch 4000 Loss 0.9113 Accuracy 0.5061\n",
            "Epoch 8 Batch 4050 Loss 0.9102 Accuracy 0.5064\n",
            "Epoch 8 Batch 4100 Loss 0.9091 Accuracy 0.5066\n",
            "Epoch 8 Batch 4150 Loss 0.9090 Accuracy 0.5067\n",
            "Epoch 8 Batch 4200 Loss 0.9089 Accuracy 0.5067\n",
            "Epoch 8 Batch 4250 Loss 0.9090 Accuracy 0.5067\n",
            "Epoch 8 Batch 4300 Loss 0.9100 Accuracy 0.5065\n",
            "Epoch 8 Batch 4350 Loss 0.9112 Accuracy 0.5064\n",
            "Epoch 8 Batch 4400 Loss 0.9123 Accuracy 0.5062\n",
            "Epoch 8 Batch 4450 Loss 0.9135 Accuracy 0.5060\n",
            "Epoch 8 Batch 4500 Loss 0.9146 Accuracy 0.5059\n",
            "Epoch 8 Batch 4550 Loss 0.9159 Accuracy 0.5057\n",
            "Epoch 8 Batch 4600 Loss 0.9173 Accuracy 0.5055\n",
            "Epoch 8 Batch 4650 Loss 0.9186 Accuracy 0.5052\n",
            "Epoch 8 Batch 4700 Loss 0.9198 Accuracy 0.5050\n",
            "Epoch 8 Batch 4750 Loss 0.9211 Accuracy 0.5048\n",
            "Epoch 8 Batch 4800 Loss 0.9222 Accuracy 0.5046\n",
            "Epoch 8 Batch 4850 Loss 0.9235 Accuracy 0.5044\n",
            "Epoch 8 Batch 4900 Loss 0.9248 Accuracy 0.5043\n",
            "Epoch 8 Batch 4950 Loss 0.9264 Accuracy 0.5041\n",
            "Epoch 8 Batch 5000 Loss 0.9278 Accuracy 0.5039\n",
            "Epoch 8 Batch 5050 Loss 0.9290 Accuracy 0.5037\n",
            "Epoch 8 Batch 5100 Loss 0.9303 Accuracy 0.5034\n",
            "Epoch 8 Batch 5150 Loss 0.9316 Accuracy 0.5032\n",
            "Epoch 8 Batch 5200 Loss 0.9325 Accuracy 0.5030\n",
            "Epoch 8 Batch 5250 Loss 0.9339 Accuracy 0.5027\n",
            "Epoch 8 Batch 5300 Loss 0.9351 Accuracy 0.5024\n",
            "Epoch 8 Batch 5350 Loss 0.9362 Accuracy 0.5021\n",
            "Epoch 8 Batch 5400 Loss 0.9371 Accuracy 0.5019\n",
            "Epoch 8 Batch 5450 Loss 0.9382 Accuracy 0.5016\n",
            "Epoch 8 Batch 5500 Loss 0.9392 Accuracy 0.5014\n",
            "Epoch 8 Batch 5550 Loss 0.9402 Accuracy 0.5011\n",
            "Epoch 8 Batch 5600 Loss 0.9412 Accuracy 0.5008\n",
            "Epoch 8 Batch 5650 Loss 0.9422 Accuracy 0.5006\n",
            "Epoch 8 Batch 5700 Loss 0.9431 Accuracy 0.5003\n",
            "Saving checkpoint for epoch 8 at /content/drive/My Drive/Transformer/cktps/ckpt-8\n",
            "Time taken for 1 epoch: 1433.8221905231476 secs\n",
            "\n",
            "Start of epoch 9\n",
            "Epoch 9 Batch 0 Loss 0.9463 Accuracy 0.4918\n",
            "Epoch 9 Batch 50 Loss 1.0586 Accuracy 0.4789\n",
            "Epoch 9 Batch 100 Loss 1.0539 Accuracy 0.4824\n",
            "Epoch 9 Batch 150 Loss 1.0466 Accuracy 0.4849\n",
            "Epoch 9 Batch 200 Loss 1.0435 Accuracy 0.4832\n",
            "Epoch 9 Batch 250 Loss 1.0438 Accuracy 0.4835\n",
            "Epoch 9 Batch 300 Loss 1.0413 Accuracy 0.4839\n",
            "Epoch 9 Batch 350 Loss 1.0404 Accuracy 0.4840\n",
            "Epoch 9 Batch 400 Loss 1.0403 Accuracy 0.4842\n",
            "Epoch 9 Batch 450 Loss 1.0349 Accuracy 0.4845\n",
            "Epoch 9 Batch 500 Loss 1.0315 Accuracy 0.4840\n",
            "Epoch 9 Batch 550 Loss 1.0296 Accuracy 0.4838\n",
            "Epoch 9 Batch 600 Loss 1.0287 Accuracy 0.4842\n",
            "Epoch 9 Batch 650 Loss 1.0279 Accuracy 0.4841\n",
            "Epoch 9 Batch 700 Loss 1.0273 Accuracy 0.4850\n",
            "Epoch 9 Batch 750 Loss 1.0258 Accuracy 0.4851\n",
            "Epoch 9 Batch 800 Loss 1.0229 Accuracy 0.4854\n",
            "Epoch 9 Batch 850 Loss 1.0217 Accuracy 0.4853\n",
            "Epoch 9 Batch 900 Loss 1.0214 Accuracy 0.4854\n",
            "Epoch 9 Batch 950 Loss 1.0200 Accuracy 0.4855\n",
            "Epoch 9 Batch 1000 Loss 1.0185 Accuracy 0.4855\n",
            "Epoch 9 Batch 1050 Loss 1.0171 Accuracy 0.4857\n",
            "Epoch 9 Batch 1100 Loss 1.0162 Accuracy 0.4861\n",
            "Epoch 9 Batch 1150 Loss 1.0151 Accuracy 0.4863\n",
            "Epoch 9 Batch 1200 Loss 1.0137 Accuracy 0.4865\n",
            "Epoch 9 Batch 1250 Loss 1.0122 Accuracy 0.4869\n",
            "Epoch 9 Batch 1300 Loss 1.0088 Accuracy 0.4874\n",
            "Epoch 9 Batch 1350 Loss 1.0073 Accuracy 0.4879\n",
            "Epoch 9 Batch 1400 Loss 1.0046 Accuracy 0.4887\n",
            "Epoch 9 Batch 1450 Loss 1.0028 Accuracy 0.4893\n",
            "Epoch 9 Batch 1500 Loss 0.9995 Accuracy 0.4903\n",
            "Epoch 9 Batch 1550 Loss 0.9973 Accuracy 0.4911\n",
            "Epoch 9 Batch 1600 Loss 0.9949 Accuracy 0.4918\n",
            "Epoch 9 Batch 1650 Loss 0.9924 Accuracy 0.4926\n",
            "Epoch 9 Batch 1700 Loss 0.9901 Accuracy 0.4934\n",
            "Epoch 9 Batch 1750 Loss 0.9881 Accuracy 0.4942\n",
            "Epoch 9 Batch 1800 Loss 0.9857 Accuracy 0.4951\n",
            "Epoch 9 Batch 1850 Loss 0.9828 Accuracy 0.4959\n",
            "Epoch 9 Batch 1900 Loss 0.9807 Accuracy 0.4967\n",
            "Epoch 9 Batch 1950 Loss 0.9788 Accuracy 0.4976\n",
            "Epoch 9 Batch 2000 Loss 0.9769 Accuracy 0.4982\n",
            "Epoch 9 Batch 2050 Loss 0.9749 Accuracy 0.4987\n",
            "Epoch 9 Batch 2100 Loss 0.9718 Accuracy 0.4991\n",
            "Epoch 9 Batch 2150 Loss 0.9688 Accuracy 0.4993\n",
            "Epoch 9 Batch 2200 Loss 0.9656 Accuracy 0.4994\n",
            "Epoch 9 Batch 2250 Loss 0.9624 Accuracy 0.4997\n",
            "Epoch 9 Batch 2300 Loss 0.9595 Accuracy 0.4999\n",
            "Epoch 9 Batch 2350 Loss 0.9564 Accuracy 0.5001\n",
            "Epoch 9 Batch 2400 Loss 0.9539 Accuracy 0.5002\n",
            "Epoch 9 Batch 2450 Loss 0.9512 Accuracy 0.5004\n",
            "Epoch 9 Batch 2500 Loss 0.9485 Accuracy 0.5006\n",
            "Epoch 9 Batch 2550 Loss 0.9459 Accuracy 0.5009\n",
            "Epoch 9 Batch 2600 Loss 0.9434 Accuracy 0.5011\n",
            "Epoch 9 Batch 2650 Loss 0.9409 Accuracy 0.5014\n",
            "Epoch 9 Batch 2700 Loss 0.9385 Accuracy 0.5018\n",
            "Epoch 9 Batch 2750 Loss 0.9362 Accuracy 0.5021\n",
            "Epoch 9 Batch 2800 Loss 0.9342 Accuracy 0.5023\n",
            "Epoch 9 Batch 2850 Loss 0.9322 Accuracy 0.5026\n",
            "Epoch 9 Batch 2900 Loss 0.9304 Accuracy 0.5029\n",
            "Epoch 9 Batch 2950 Loss 0.9285 Accuracy 0.5032\n",
            "Epoch 9 Batch 3000 Loss 0.9265 Accuracy 0.5034\n",
            "Epoch 9 Batch 3050 Loss 0.9246 Accuracy 0.5037\n",
            "Epoch 9 Batch 3100 Loss 0.9227 Accuracy 0.5039\n",
            "Epoch 9 Batch 3150 Loss 0.9208 Accuracy 0.5041\n",
            "Epoch 9 Batch 3200 Loss 0.9188 Accuracy 0.5044\n",
            "Epoch 9 Batch 3250 Loss 0.9168 Accuracy 0.5046\n",
            "Epoch 9 Batch 3300 Loss 0.9148 Accuracy 0.5049\n",
            "Epoch 9 Batch 3350 Loss 0.9129 Accuracy 0.5052\n",
            "Epoch 9 Batch 3400 Loss 0.9110 Accuracy 0.5054\n",
            "Epoch 9 Batch 3450 Loss 0.9093 Accuracy 0.5056\n",
            "Epoch 9 Batch 3500 Loss 0.9074 Accuracy 0.5059\n",
            "Epoch 9 Batch 3550 Loss 0.9058 Accuracy 0.5062\n",
            "Epoch 9 Batch 3600 Loss 0.9040 Accuracy 0.5065\n",
            "Epoch 9 Batch 3650 Loss 0.9028 Accuracy 0.5068\n",
            "Epoch 9 Batch 3700 Loss 0.9012 Accuracy 0.5072\n",
            "Epoch 9 Batch 3750 Loss 0.8997 Accuracy 0.5075\n",
            "Epoch 9 Batch 3800 Loss 0.8983 Accuracy 0.5078\n",
            "Epoch 9 Batch 3850 Loss 0.8968 Accuracy 0.5081\n",
            "Epoch 9 Batch 3900 Loss 0.8958 Accuracy 0.5084\n",
            "Epoch 9 Batch 3950 Loss 0.8945 Accuracy 0.5086\n",
            "Epoch 9 Batch 4000 Loss 0.8932 Accuracy 0.5090\n",
            "Epoch 9 Batch 4050 Loss 0.8921 Accuracy 0.5093\n",
            "Epoch 9 Batch 4100 Loss 0.8911 Accuracy 0.5095\n",
            "Epoch 9 Batch 4150 Loss 0.8908 Accuracy 0.5096\n",
            "Epoch 9 Batch 4200 Loss 0.8909 Accuracy 0.5096\n",
            "Epoch 9 Batch 4250 Loss 0.8914 Accuracy 0.5095\n",
            "Epoch 9 Batch 4300 Loss 0.8923 Accuracy 0.5094\n",
            "Epoch 9 Batch 4350 Loss 0.8932 Accuracy 0.5093\n",
            "Epoch 9 Batch 4400 Loss 0.8944 Accuracy 0.5092\n",
            "Epoch 9 Batch 4450 Loss 0.8953 Accuracy 0.5090\n",
            "Epoch 9 Batch 4500 Loss 0.8963 Accuracy 0.5088\n",
            "Epoch 9 Batch 4550 Loss 0.8977 Accuracy 0.5086\n",
            "Epoch 9 Batch 4600 Loss 0.8990 Accuracy 0.5084\n",
            "Epoch 9 Batch 4650 Loss 0.9005 Accuracy 0.5083\n",
            "Epoch 9 Batch 4700 Loss 0.9022 Accuracy 0.5081\n",
            "Epoch 9 Batch 4750 Loss 0.9036 Accuracy 0.5080\n",
            "Epoch 9 Batch 4800 Loss 0.9049 Accuracy 0.5078\n",
            "Epoch 9 Batch 4850 Loss 0.9060 Accuracy 0.5076\n",
            "Epoch 9 Batch 4900 Loss 0.9073 Accuracy 0.5074\n",
            "Epoch 9 Batch 4950 Loss 0.9083 Accuracy 0.5072\n",
            "Epoch 9 Batch 5000 Loss 0.9097 Accuracy 0.5070\n",
            "Epoch 9 Batch 5050 Loss 0.9112 Accuracy 0.5068\n",
            "Epoch 9 Batch 5100 Loss 0.9126 Accuracy 0.5065\n",
            "Epoch 9 Batch 5150 Loss 0.9139 Accuracy 0.5063\n",
            "Epoch 9 Batch 5200 Loss 0.9153 Accuracy 0.5060\n",
            "Epoch 9 Batch 5250 Loss 0.9167 Accuracy 0.5058\n",
            "Epoch 9 Batch 5300 Loss 0.9175 Accuracy 0.5055\n",
            "Epoch 9 Batch 5350 Loss 0.9186 Accuracy 0.5052\n",
            "Epoch 9 Batch 5400 Loss 0.9196 Accuracy 0.5049\n",
            "Epoch 9 Batch 5450 Loss 0.9208 Accuracy 0.5046\n",
            "Epoch 9 Batch 5500 Loss 0.9218 Accuracy 0.5043\n",
            "Epoch 9 Batch 5550 Loss 0.9230 Accuracy 0.5041\n",
            "Epoch 9 Batch 5600 Loss 0.9239 Accuracy 0.5038\n",
            "Epoch 9 Batch 5650 Loss 0.9248 Accuracy 0.5036\n",
            "Epoch 9 Batch 5700 Loss 0.9257 Accuracy 0.5033\n",
            "Saving checkpoint for epoch 9 at /content/drive/My Drive/Transformer/cktps/ckpt-9\n",
            "Time taken for 1 epoch: 1432.6717646121979 secs\n",
            "\n",
            "Start of epoch 10\n",
            "Epoch 10 Batch 0 Loss 1.2093 Accuracy 0.4720\n",
            "Epoch 10 Batch 50 Loss 1.0154 Accuracy 0.4807\n",
            "Epoch 10 Batch 100 Loss 1.0214 Accuracy 0.4836\n",
            "Epoch 10 Batch 150 Loss 1.0166 Accuracy 0.4840\n",
            "Epoch 10 Batch 200 Loss 1.0193 Accuracy 0.4854\n",
            "Epoch 10 Batch 250 Loss 1.0207 Accuracy 0.4865\n",
            "Epoch 10 Batch 300 Loss 1.0168 Accuracy 0.4868\n",
            "Epoch 10 Batch 350 Loss 1.0160 Accuracy 0.4869\n",
            "Epoch 10 Batch 400 Loss 1.0182 Accuracy 0.4867\n",
            "Epoch 10 Batch 450 Loss 1.0159 Accuracy 0.4873\n",
            "Epoch 10 Batch 500 Loss 1.0153 Accuracy 0.4876\n",
            "Epoch 10 Batch 550 Loss 1.0148 Accuracy 0.4873\n",
            "Epoch 10 Batch 600 Loss 1.0122 Accuracy 0.4873\n",
            "Epoch 10 Batch 650 Loss 1.0117 Accuracy 0.4875\n",
            "Epoch 10 Batch 700 Loss 1.0108 Accuracy 0.4879\n",
            "Epoch 10 Batch 750 Loss 1.0087 Accuracy 0.4885\n",
            "Epoch 10 Batch 800 Loss 1.0077 Accuracy 0.4886\n",
            "Epoch 10 Batch 850 Loss 1.0073 Accuracy 0.4888\n",
            "Epoch 10 Batch 900 Loss 1.0073 Accuracy 0.4882\n",
            "Epoch 10 Batch 950 Loss 1.0067 Accuracy 0.4883\n",
            "Epoch 10 Batch 1000 Loss 1.0043 Accuracy 0.4884\n",
            "Epoch 10 Batch 1050 Loss 1.0028 Accuracy 0.4884\n",
            "Epoch 10 Batch 1100 Loss 1.0013 Accuracy 0.4886\n",
            "Epoch 10 Batch 1150 Loss 0.9999 Accuracy 0.4890\n",
            "Epoch 10 Batch 1200 Loss 0.9979 Accuracy 0.4893\n",
            "Epoch 10 Batch 1250 Loss 0.9952 Accuracy 0.4895\n",
            "Epoch 10 Batch 1300 Loss 0.9932 Accuracy 0.4901\n",
            "Epoch 10 Batch 1350 Loss 0.9907 Accuracy 0.4905\n",
            "Epoch 10 Batch 1400 Loss 0.9889 Accuracy 0.4912\n",
            "Epoch 10 Batch 1450 Loss 0.9866 Accuracy 0.4919\n",
            "Epoch 10 Batch 1500 Loss 0.9839 Accuracy 0.4928\n",
            "Epoch 10 Batch 1550 Loss 0.9809 Accuracy 0.4935\n",
            "Epoch 10 Batch 1600 Loss 0.9785 Accuracy 0.4943\n",
            "Epoch 10 Batch 1650 Loss 0.9755 Accuracy 0.4951\n",
            "Epoch 10 Batch 1700 Loss 0.9734 Accuracy 0.4958\n",
            "Epoch 10 Batch 1750 Loss 0.9702 Accuracy 0.4967\n",
            "Epoch 10 Batch 1800 Loss 0.9683 Accuracy 0.4976\n",
            "Epoch 10 Batch 1850 Loss 0.9661 Accuracy 0.4985\n",
            "Epoch 10 Batch 1900 Loss 0.9643 Accuracy 0.4993\n",
            "Epoch 10 Batch 1950 Loss 0.9627 Accuracy 0.5000\n",
            "Epoch 10 Batch 2000 Loss 0.9605 Accuracy 0.5006\n",
            "Epoch 10 Batch 2050 Loss 0.9581 Accuracy 0.5012\n",
            "Epoch 10 Batch 2100 Loss 0.9556 Accuracy 0.5014\n",
            "Epoch 10 Batch 2150 Loss 0.9531 Accuracy 0.5015\n",
            "Epoch 10 Batch 2200 Loss 0.9504 Accuracy 0.5017\n",
            "Epoch 10 Batch 2250 Loss 0.9472 Accuracy 0.5018\n",
            "Epoch 10 Batch 2300 Loss 0.9442 Accuracy 0.5019\n",
            "Epoch 10 Batch 2350 Loss 0.9415 Accuracy 0.5022\n",
            "Epoch 10 Batch 2400 Loss 0.9389 Accuracy 0.5024\n",
            "Epoch 10 Batch 2450 Loss 0.9363 Accuracy 0.5027\n",
            "Epoch 10 Batch 2500 Loss 0.9339 Accuracy 0.5030\n",
            "Epoch 10 Batch 2550 Loss 0.9310 Accuracy 0.5033\n",
            "Epoch 10 Batch 2600 Loss 0.9284 Accuracy 0.5035\n",
            "Epoch 10 Batch 2650 Loss 0.9260 Accuracy 0.5038\n",
            "Epoch 10 Batch 2700 Loss 0.9233 Accuracy 0.5042\n",
            "Epoch 10 Batch 2750 Loss 0.9206 Accuracy 0.5044\n",
            "Epoch 10 Batch 2800 Loss 0.9188 Accuracy 0.5047\n",
            "Epoch 10 Batch 2850 Loss 0.9170 Accuracy 0.5049\n",
            "Epoch 10 Batch 2900 Loss 0.9151 Accuracy 0.5051\n",
            "Epoch 10 Batch 2950 Loss 0.9132 Accuracy 0.5056\n",
            "Epoch 10 Batch 3000 Loss 0.9119 Accuracy 0.5058\n",
            "Epoch 10 Batch 3050 Loss 0.9099 Accuracy 0.5061\n",
            "Epoch 10 Batch 3100 Loss 0.9080 Accuracy 0.5064\n",
            "Epoch 10 Batch 3150 Loss 0.9062 Accuracy 0.5066\n",
            "Epoch 10 Batch 3200 Loss 0.9045 Accuracy 0.5068\n",
            "Epoch 10 Batch 3250 Loss 0.9028 Accuracy 0.5070\n",
            "Epoch 10 Batch 3300 Loss 0.9007 Accuracy 0.5072\n",
            "Epoch 10 Batch 3350 Loss 0.8988 Accuracy 0.5075\n",
            "Epoch 10 Batch 3400 Loss 0.8969 Accuracy 0.5077\n",
            "Epoch 10 Batch 3450 Loss 0.8952 Accuracy 0.5080\n",
            "Epoch 10 Batch 3500 Loss 0.8934 Accuracy 0.5082\n",
            "Epoch 10 Batch 3550 Loss 0.8915 Accuracy 0.5086\n",
            "Epoch 10 Batch 3600 Loss 0.8899 Accuracy 0.5090\n",
            "Epoch 10 Batch 3650 Loss 0.8880 Accuracy 0.5093\n",
            "Epoch 10 Batch 3700 Loss 0.8865 Accuracy 0.5097\n",
            "Epoch 10 Batch 3750 Loss 0.8849 Accuracy 0.5100\n",
            "Epoch 10 Batch 3800 Loss 0.8834 Accuracy 0.5103\n",
            "Epoch 10 Batch 3850 Loss 0.8820 Accuracy 0.5105\n",
            "Epoch 10 Batch 3900 Loss 0.8806 Accuracy 0.5108\n",
            "Epoch 10 Batch 3950 Loss 0.8793 Accuracy 0.5111\n",
            "Epoch 10 Batch 4000 Loss 0.8778 Accuracy 0.5115\n",
            "Epoch 10 Batch 4050 Loss 0.8764 Accuracy 0.5117\n",
            "Epoch 10 Batch 4100 Loss 0.8756 Accuracy 0.5120\n",
            "Epoch 10 Batch 4150 Loss 0.8754 Accuracy 0.5121\n",
            "Epoch 10 Batch 4200 Loss 0.8755 Accuracy 0.5121\n",
            "Epoch 10 Batch 4250 Loss 0.8762 Accuracy 0.5120\n",
            "Epoch 10 Batch 4300 Loss 0.8771 Accuracy 0.5119\n",
            "Epoch 10 Batch 4350 Loss 0.8781 Accuracy 0.5118\n",
            "Epoch 10 Batch 4400 Loss 0.8790 Accuracy 0.5116\n",
            "Epoch 10 Batch 4450 Loss 0.8804 Accuracy 0.5114\n",
            "Epoch 10 Batch 4500 Loss 0.8816 Accuracy 0.5112\n",
            "Epoch 10 Batch 4550 Loss 0.8833 Accuracy 0.5110\n",
            "Epoch 10 Batch 4600 Loss 0.8847 Accuracy 0.5108\n",
            "Epoch 10 Batch 4650 Loss 0.8863 Accuracy 0.5106\n",
            "Epoch 10 Batch 4700 Loss 0.8877 Accuracy 0.5104\n",
            "Epoch 10 Batch 4750 Loss 0.8891 Accuracy 0.5103\n",
            "Epoch 10 Batch 4800 Loss 0.8903 Accuracy 0.5100\n",
            "Epoch 10 Batch 4850 Loss 0.8914 Accuracy 0.5099\n",
            "Epoch 10 Batch 4900 Loss 0.8928 Accuracy 0.5097\n",
            "Epoch 10 Batch 4950 Loss 0.8941 Accuracy 0.5095\n",
            "Epoch 10 Batch 5000 Loss 0.8950 Accuracy 0.5092\n",
            "Epoch 10 Batch 5050 Loss 0.8964 Accuracy 0.5090\n",
            "Epoch 10 Batch 5100 Loss 0.8977 Accuracy 0.5088\n",
            "Epoch 10 Batch 5150 Loss 0.8992 Accuracy 0.5086\n",
            "Epoch 10 Batch 5200 Loss 0.9006 Accuracy 0.5083\n",
            "Epoch 10 Batch 5250 Loss 0.9018 Accuracy 0.5080\n",
            "Epoch 10 Batch 5300 Loss 0.9030 Accuracy 0.5077\n",
            "Epoch 10 Batch 5350 Loss 0.9044 Accuracy 0.5074\n",
            "Epoch 10 Batch 5400 Loss 0.9052 Accuracy 0.5071\n",
            "Epoch 10 Batch 5450 Loss 0.9063 Accuracy 0.5068\n",
            "Epoch 10 Batch 5500 Loss 0.9074 Accuracy 0.5066\n",
            "Epoch 10 Batch 5550 Loss 0.9085 Accuracy 0.5063\n",
            "Epoch 10 Batch 5600 Loss 0.9095 Accuracy 0.5060\n",
            "Epoch 10 Batch 5650 Loss 0.9105 Accuracy 0.5058\n",
            "Epoch 10 Batch 5700 Loss 0.9112 Accuracy 0.5056\n",
            "Saving checkpoint for epoch 10 at /content/drive/My Drive/Transformer/cktps/ckpt-10\n",
            "Time taken for 1 epoch: 1436.9990768432617 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djU95U3HzTVC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 20\n",
        "def evaluate(inp_sentence):\n",
        "    inp_sentence = \\\n",
        "        [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
        "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
        "    \n",
        "    output = tf.expand_dims([VOCAB_SIZE_FR-2], axis=0)\n",
        "    \n",
        "    for _ in range(MAX_LENGTH):\n",
        "        predictions = transformer(enc_input, output, False)\n",
        "        \n",
        "        prediction = predictions[:, -1:, :]\n",
        "        \n",
        "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "        \n",
        "        if predicted_id == VOCAB_SIZE_FR-1:\n",
        "            return tf.squeeze(output, axis=0)\n",
        "        \n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "        \n",
        "    return tf.squeeze(output, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOPqJyOAf1UA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "    output = evaluate(sentence).numpy()\n",
        "    \n",
        "    predicted_sentence = tokenizer_fr.decode(\n",
        "        [i for i in output if i < VOCAB_SIZE_FR-2]\n",
        "    )\n",
        "    \n",
        "    print(\"Input: {}\".format(sentence))\n",
        "    print(\"Predicted translation: {}\".format(predicted_sentence))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6S1zNhcyf3p2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "e3b8ee98-5217-40f8-da23-6d1536fe4079"
      },
      "source": [
        "translate(\"This is a really powerful tool!\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: This is a really powerful tool!\n",
            "Predicted translation: C'est un instrument vraiment puissant!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uc_tyup3f4lI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}